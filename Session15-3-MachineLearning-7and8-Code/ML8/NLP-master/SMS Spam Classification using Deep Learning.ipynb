{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#latin - 1 is the universal encoding of the text\n",
    "data = pd.read_csv(\"spamdata.csv\",encoding=\"latin-1\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Data Preprocesing or Text Cleaning\n",
    "def clean_text(text):\n",
    "    # convert to lower case\n",
    "    cleaned_text = text.lower()\n",
    "    # remove punctuations\n",
    "    cleaned_text = \"\".join(c for c in cleaned_text if c not in string.punctuation)\n",
    "    #remove stopwords\n",
    "    words = [word for word in cleaned_text.split() if word not in stopwords.words(\"english\")]\n",
    "    #lemmatization - context in which the word used retained\n",
    "    # need to specify POS tag for lemmatization not for stemming\n",
    "    words = [lem.lemmatize(word,\"v\") for word in words]\n",
    "    words = [lem.lemmatize(word,\"n\") for word in words]\n",
    "    #join the cleaned words\n",
    "    cleaned_text = \" \".join(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play game today'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"I will be playing a game today !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                              ok lar joke wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf live around though  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned'] = data['text'].apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed data - cleaned.\n",
    "\n",
    "It can be further extended to correction of spellings, removal of numbers in the text etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta Features - counts / attributes associated with text data\n",
    "data[\"word_count\"] = data[\"text\"].apply(lambda x : len(x.split()))\n",
    "data[\"word_count_cleaned\"] = data[\"cleaned\"].apply(lambda x : len(x.split()))\n",
    "\n",
    "## character count\n",
    "data[\"char_count\"] = data[\"text\"].apply(lambda x: len(x))\n",
    "data[\"char_count_without_spaces\"] = data[\"text\"].apply(lambda x: len(x.replace(\" \",\"\")))\n",
    "\n",
    "## sum of number of digits present in each text\n",
    "data[\"num_digit\"] = data[\"text\"].apply(lambda x : sum([1 if w.isdigit() else 0 for w in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_cleaned</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_without_spaces</th>\n",
       "      <th>num_digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  word_count  \\\n",
       "0  go jurong point crazy available bugis n great ...          20   \n",
       "1                              ok lar joke wif u oni           6   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...          28   \n",
       "3                u dun say early hor u c already say          11   \n",
       "4           nah dont think go usf live around though          13   \n",
       "\n",
       "   word_count_cleaned  char_count  char_count_without_spaces  num_digit  \n",
       "0                  16         111                         92          0  \n",
       "1                   6          29                         24          0  \n",
       "2                  23         155                        128          2  \n",
       "3                   9          49                         39          0  \n",
       "4                   8          61                         49          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#meta faetures can be used for any ML inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of nouns or verbs present in the data. It can also be extended for pronous,Adjective or adverb\n",
    "pos_dic = {\"noun\":[\"NNP\",\"NN\",\"NNS\",\"NNPS\"],\"verb\":[\"VBZ\",\"VB\",\"VBD\",\"VBN\",\"VBG\",\"VBP\"]}\n",
    "def pos_check(text,family):\n",
    "    #annotate text by pos\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    count = 0\n",
    "    #print(\"Tags \")\n",
    "    #print(\"=\"*10)\n",
    "    for tag in tags:\n",
    "        tag = tag[1]\n",
    "        #print(tag)\n",
    "        if tag in pos_dic[family]:\n",
    "            count +=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_check(\"They are playing in the ground\",\"noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_check(\"They are playing in the ground\",\"verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pos_noun_count\"] = data[\"text\"].apply(lambda x : pos_check(x,\"noun\"))\n",
    "data[\"pos_verb_count\"] = data[\"text\"].apply(lambda x : pos_check(x,\"verb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_cleaned</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_without_spaces</th>\n",
       "      <th>num_digit</th>\n",
       "      <th>pos_noun_count</th>\n",
       "      <th>pos_verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  word_count  \\\n",
       "0  go jurong point crazy available bugis n great ...          20   \n",
       "1                              ok lar joke wif u oni           6   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...          28   \n",
       "3                u dun say early hor u c already say          11   \n",
       "4           nah dont think go usf live around though          13   \n",
       "\n",
       "   word_count_cleaned  char_count  char_count_without_spaces  num_digit  \\\n",
       "0                  16         111                         92          0   \n",
       "1                   6          29                         24          0   \n",
       "2                  23         155                        128          2   \n",
       "3                   9          49                         39          0   \n",
       "4                   8          61                         49          0   \n",
       "\n",
       "   pos_noun_count  pos_verb_count  \n",
       "0              10               1  \n",
       "1               4               0  \n",
       "2              13               4  \n",
       "3               3               3  \n",
       "4               1               5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 10 columns):\n",
      "label                        5572 non-null object\n",
      "text                         5572 non-null object\n",
      "cleaned                      5572 non-null object\n",
      "word_count                   5572 non-null int64\n",
      "word_count_cleaned           5572 non-null int64\n",
      "char_count                   5572 non-null int64\n",
      "char_count_without_spaces    5572 non-null int64\n",
      "num_digit                    5572 non-null int64\n",
      "pos_noun_count               5572 non-null int64\n",
      "pos_verb_count               5572 non-null int64\n",
      "dtypes: int64(7), object(3)\n",
      "memory usage: 435.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count as a feature -count of words in a document or corpus\n",
    "#count features\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cvz = CountVectorizer()\n",
    "cvz.fit(data[\"cleaned\"].values)#finf corresponding counts\n",
    "count_vectors = cvz.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8206 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 46827 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tfidf = TfidfVectorizer(max_features=500)\n",
    "word_tfidf.fit(data[\"cleaned\"].values)#finf corresponding counts\n",
    "word_vectors_tfidf = word_tfidf.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28313 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram level tfidf_features\n",
    "#instaed of stacking word tfidf we can stack ngram tfidf\n",
    "#represent max features=500 and bigram\n",
    "ngram_tfidf = TfidfVectorizer(max_features=500, ngram_range=(1,2))\n",
    "ngram_tfidf.fit(data[\"cleaned\"].values)#finf corresponding counts\n",
    "ngram_vectors_tfidf = ngram_tfidf.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28613 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectors_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### another variation character level tfidf-analyzer = \"char\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tfidf = TfidfVectorizer(max_features=500, analyzer=\"char\")\n",
    "char_tfidf.fit(data[\"cleaned\"].values)#finf corresponding counts\n",
    "char_vectors_tfidf = char_tfidf.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x65 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 97087 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectors_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip essentail keyword features / words with their corresponding tfidf counts\n",
    "tfidf = dict(zip(word_tfidf.get_feature_names(),word_tfidf.idf_))\n",
    "tf_idf = pd.DataFrame(columns=[\"word_tfidf\"]).from_dict(tfidf,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0089my</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585236</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585334</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0125698789</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020603</th>\n",
       "      <td>8.016251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0207</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02070836089</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02072069400</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02073162414</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02085076972</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020903</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>021</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>050703</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0578</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>060505</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>061104</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07008009200</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07046744435</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07090201529</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07090298926</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07099833605</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>071104</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07123456789</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0721072</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07732584351</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07734396839</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoyyooo</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>6.581166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ystrdayice</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummmm</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummy</th>\n",
       "      <td>8.239394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yun</th>\n",
       "      <td>7.833929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yunny</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuo</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuou</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yup</th>\n",
       "      <td>5.841499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yupz</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ywhere</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zac</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zahers</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealand</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zed</th>\n",
       "      <td>7.679779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zhong</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zindgi</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoe</th>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zogtorius</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zouk</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyada</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>¹ã</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>âªm</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>âªt</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>âªve</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â¼120</th>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word_tfidf\n",
       "008704050406    8.527076\n",
       "0089my          8.932542\n",
       "0121            8.932542\n",
       "01223585236     8.932542\n",
       "01223585334     8.527076\n",
       "0125698789      8.932542\n",
       "02              8.932542\n",
       "020603          8.016251\n",
       "0207            8.527076\n",
       "02070836089     8.932542\n",
       "02072069400     8.932542\n",
       "02073162414     8.527076\n",
       "02085076972     8.932542\n",
       "020903          8.527076\n",
       "021             8.527076\n",
       "050703          8.527076\n",
       "0578            8.527076\n",
       "06              8.932542\n",
       "060505          8.932542\n",
       "061104          8.932542\n",
       "07008009200     8.932542\n",
       "07046744435     8.932542\n",
       "07090201529     8.932542\n",
       "07090298926     8.932542\n",
       "07099833605     8.932542\n",
       "071104          8.527076\n",
       "07123456789     8.527076\n",
       "0721072         8.932542\n",
       "07732584351     8.932542\n",
       "07734396839     8.527076\n",
       "...                  ...\n",
       "yoyyooo         8.932542\n",
       "yr              6.581166\n",
       "ystrdayice      8.932542\n",
       "yummmm          8.932542\n",
       "yummy           8.239394\n",
       "yun             7.833929\n",
       "yunny           8.527076\n",
       "yuo             8.527076\n",
       "yuou            8.932542\n",
       "yup             5.841499\n",
       "yupz            8.932542\n",
       "ywhere          8.932542\n",
       "zac             8.932542\n",
       "zahers          8.932542\n",
       "zealand         8.932542\n",
       "zebra           8.932542\n",
       "zed             7.679779\n",
       "zero            8.932542\n",
       "zhong           8.932542\n",
       "zindgi          8.932542\n",
       "zoe             8.527076\n",
       "zogtorius       8.932542\n",
       "zoom            8.932542\n",
       "zouk            8.932542\n",
       "zyada           8.932542\n",
       "¹ã              8.932542\n",
       "âªm             8.932542\n",
       "âªt             8.932542\n",
       "âªve            8.932542\n",
       "â¼120           8.932542\n",
       "\n",
       "[8206 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.columns=[\"word_tfidf\"]\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features - frequency of words,nlp based features like pos/metafeatures - count of words, count of words etc.\n",
    "#but to use them we have to create a matrix and then apply respective ML models for classifictaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'cleaned', 'word_count', 'word_count_cleaned',\n",
       "       'char_count', 'char_count_without_spaces', 'num_digit',\n",
       "       'pos_noun_count', 'pos_verb_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sparse matrix horizontal placement\n",
    "from scipy.sparse import hstack,csr_matrix\n",
    "\n",
    "meta_features = ['word_count', 'word_count_cleaned',\n",
    "       'char_count', 'char_count_without_spaces', 'num_digit',\n",
    "       'pos_noun_count', 'pos_verb_count']\n",
    "\n",
    "#data corresponding to meta features contained in feature_set 1\n",
    "feature_set1 = data[meta_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert feature_set into a matrix\n",
    "#also join idf values\n",
    "train = hstack([word_vectors_tfidf,csr_matrix(feature_set1)],\"csr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 507)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8213 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 80609 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify if a give text is a spam or not\n",
    "#label encode target variable as they are spam and ham now\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "target = data[\"label\"].values\n",
    "target = LabelEncoder().fit_transform(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(train,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179, 507), (4179,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1393, 507), (1393,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score :  97.48743718592965\n"
     ]
    }
   ],
   "source": [
    "model = naive_bayes.MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "preds_nb = model.predict(X_test)\n",
    "acc_nb = accuracy_score(y_test,preds_nb)\n",
    "print(\"Naive Bayes Accuracy Score : \",acc_nb*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score :  97.5592246949031\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "preds_LR = model.predict(X_test)\n",
    "acc_LR = accuracy_score(y_test,preds_LR)\n",
    "print(\"Logistic Regression Accuracy Score : \",acc_LR*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score :  93.3237616654702\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(X_train,y_train)\n",
    "preds_svm = model.predict(X_test)\n",
    "acc_svm = accuracy_score(y_test,preds_svm)\n",
    "print(\"SVM Accuracy Score : \",acc_svm*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM models require large amount of data to perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier Accuracy Score :  97.91816223977028\n"
     ]
    }
   ],
   "source": [
    "model = ensemble.ExtraTreesClassifier()#bagging\n",
    "model.fit(X_train,y_train)\n",
    "preds_bag = model.predict(X_test)\n",
    "acc_bag = accuracy_score(y_test,preds_bag)\n",
    "print(\"ExtraTreesClassifier Accuracy Score : \",acc_bag*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we do not need to go for complex models .Simple models shaoe good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy Score :  97.91816223977028\n"
     ]
    }
   ],
   "source": [
    "model = ensemble.RandomForestClassifier()#bagging\n",
    "model.fit(X_train,y_train)\n",
    "preds_rf = model.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test,preds_rf)\n",
    "print(\"Random Forest Classifier Accuracy Score : \",acc_bag*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPAM CLASSIFICATION USING DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent text in word emebeddding format\n",
    "#use pretrained emebddings\n",
    "embeddings_index = ()\n",
    "#word vectors for 2 million words\n",
    "for i , line in enumerate(open(\"pretrained.vec\",encoding=\"utf8\")):\n",
    "    if i ==0:\n",
    "        continue\n",
    "    value = line.split()\n",
    "    print(line)\n",
    "    embeddings_index[value[0]] = values[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text into word embeddings\n",
    "from keras.preprocessing import text,sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(data[\"text\"])\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'to': 2,\n",
       " 'you': 3,\n",
       " 'a': 4,\n",
       " 'the': 5,\n",
       " 'u': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'is': 9,\n",
       " 'me': 10,\n",
       " 'my': 11,\n",
       " 'for': 12,\n",
       " 'your': 13,\n",
       " 'it': 14,\n",
       " 'of': 15,\n",
       " 'call': 16,\n",
       " 'have': 17,\n",
       " 'on': 18,\n",
       " '2': 19,\n",
       " 'that': 20,\n",
       " 'now': 21,\n",
       " 'are': 22,\n",
       " 'so': 23,\n",
       " 'but': 24,\n",
       " 'not': 25,\n",
       " 'or': 26,\n",
       " 'do': 27,\n",
       " 'can': 28,\n",
       " 'at': 29,\n",
       " \"i'm\": 30,\n",
       " 'get': 31,\n",
       " 'be': 32,\n",
       " 'will': 33,\n",
       " 'if': 34,\n",
       " 'ur': 35,\n",
       " 'with': 36,\n",
       " 'just': 37,\n",
       " 'no': 38,\n",
       " 'we': 39,\n",
       " 'this': 40,\n",
       " 'gt': 41,\n",
       " '4': 42,\n",
       " 'lt': 43,\n",
       " 'up': 44,\n",
       " 'when': 45,\n",
       " 'ok': 46,\n",
       " 'free': 47,\n",
       " 'from': 48,\n",
       " 'how': 49,\n",
       " 'go': 50,\n",
       " 'all': 51,\n",
       " 'out': 52,\n",
       " 'what': 53,\n",
       " 'know': 54,\n",
       " 'like': 55,\n",
       " 'good': 56,\n",
       " 'then': 57,\n",
       " 'got': 58,\n",
       " 'was': 59,\n",
       " 'come': 60,\n",
       " 'its': 61,\n",
       " 'am': 62,\n",
       " 'time': 63,\n",
       " 'only': 64,\n",
       " 'day': 65,\n",
       " 'love': 66,\n",
       " 'there': 67,\n",
       " 'send': 68,\n",
       " 'he': 69,\n",
       " 'want': 70,\n",
       " 'text': 71,\n",
       " 'as': 72,\n",
       " 'txt': 73,\n",
       " 'one': 74,\n",
       " 'going': 75,\n",
       " 'by': 76,\n",
       " 'home': 77,\n",
       " \"i'll\": 78,\n",
       " 'need': 79,\n",
       " 'about': 80,\n",
       " 'r': 81,\n",
       " 'lor': 82,\n",
       " 'sorry': 83,\n",
       " 'stop': 84,\n",
       " 'still': 85,\n",
       " 'see': 86,\n",
       " 'back': 87,\n",
       " 'today': 88,\n",
       " 'n': 89,\n",
       " 'da': 90,\n",
       " 'our': 91,\n",
       " 'reply': 92,\n",
       " 'k': 93,\n",
       " 'dont': 94,\n",
       " 'she': 95,\n",
       " 'mobile': 96,\n",
       " 'take': 97,\n",
       " \"don't\": 98,\n",
       " 'tell': 99,\n",
       " 'hi': 100,\n",
       " 'new': 101,\n",
       " 'later': 102,\n",
       " 'her': 103,\n",
       " 'pls': 104,\n",
       " 'any': 105,\n",
       " 'please': 106,\n",
       " 'think': 107,\n",
       " 'been': 108,\n",
       " 'they': 109,\n",
       " 'phone': 110,\n",
       " 'here': 111,\n",
       " 'week': 112,\n",
       " 'dear': 113,\n",
       " 'did': 114,\n",
       " 'some': 115,\n",
       " 'ã\\x8c': 116,\n",
       " '1': 117,\n",
       " 'well': 118,\n",
       " 'has': 119,\n",
       " 'much': 120,\n",
       " 'great': 121,\n",
       " 'night': 122,\n",
       " 'oh': 123,\n",
       " 'claim': 124,\n",
       " 'an': 125,\n",
       " 'hope': 126,\n",
       " 'hey': 127,\n",
       " 'msg': 128,\n",
       " 'who': 129,\n",
       " 'him': 130,\n",
       " 'where': 131,\n",
       " 'd': 132,\n",
       " 'more': 133,\n",
       " 'too': 134,\n",
       " 'happy': 135,\n",
       " 'had': 136,\n",
       " 'yes': 137,\n",
       " 'make': 138,\n",
       " 'way': 139,\n",
       " 'c': 140,\n",
       " 'www': 141,\n",
       " 'work': 142,\n",
       " 'give': 143,\n",
       " 'wat': 144,\n",
       " \"it's\": 145,\n",
       " 'number': 146,\n",
       " 'e': 147,\n",
       " 'message': 148,\n",
       " 'should': 149,\n",
       " 'prize': 150,\n",
       " 'tomorrow': 151,\n",
       " 'say': 152,\n",
       " 'right': 153,\n",
       " 'already': 154,\n",
       " 'after': 155,\n",
       " 'ask': 156,\n",
       " 'cash': 157,\n",
       " 'doing': 158,\n",
       " 'said': 159,\n",
       " '3': 160,\n",
       " 'yeah': 161,\n",
       " 'really': 162,\n",
       " 'amp': 163,\n",
       " 'why': 164,\n",
       " 'im': 165,\n",
       " 'meet': 166,\n",
       " 'them': 167,\n",
       " 'life': 168,\n",
       " 'find': 169,\n",
       " 'very': 170,\n",
       " 'morning': 171,\n",
       " 'babe': 172,\n",
       " 'last': 173,\n",
       " 'miss': 174,\n",
       " 'thanks': 175,\n",
       " 'would': 176,\n",
       " 'cos': 177,\n",
       " 'win': 178,\n",
       " 't': 179,\n",
       " 'lol': 180,\n",
       " 'also': 181,\n",
       " 'won': 182,\n",
       " 'let': 183,\n",
       " 'b': 184,\n",
       " 'anything': 185,\n",
       " 'every': 186,\n",
       " '150p': 187,\n",
       " 'com': 188,\n",
       " 'sure': 189,\n",
       " 'pick': 190,\n",
       " 'care': 191,\n",
       " 'urgent': 192,\n",
       " 'nokia': 193,\n",
       " 'sent': 194,\n",
       " 'keep': 195,\n",
       " 'over': 196,\n",
       " 'uk': 197,\n",
       " 'something': 198,\n",
       " 'contact': 199,\n",
       " 'us': 200,\n",
       " 'again': 201,\n",
       " 'buy': 202,\n",
       " 'min': 203,\n",
       " 'wait': 204,\n",
       " 'cant': 205,\n",
       " 'before': 206,\n",
       " \"i've\": 207,\n",
       " 'first': 208,\n",
       " 's': 209,\n",
       " '5': 210,\n",
       " 'even': 211,\n",
       " 'next': 212,\n",
       " 'feel': 213,\n",
       " 'were': 214,\n",
       " 'nice': 215,\n",
       " 'went': 216,\n",
       " 'thing': 217,\n",
       " 'around': 218,\n",
       " 'soon': 219,\n",
       " 'his': 220,\n",
       " 'which': 221,\n",
       " 'someone': 222,\n",
       " \"can't\": 223,\n",
       " 'could': 224,\n",
       " 'place': 225,\n",
       " 'money': 226,\n",
       " 'service': 227,\n",
       " 'off': 228,\n",
       " 'tone': 229,\n",
       " '50': 230,\n",
       " 'tonight': 231,\n",
       " 'late': 232,\n",
       " 'many': 233,\n",
       " 'per': 234,\n",
       " 'customer': 235,\n",
       " 'gonna': 236,\n",
       " 'chat': 237,\n",
       " 'ya': 238,\n",
       " 'sleep': 239,\n",
       " 'always': 240,\n",
       " 'leave': 241,\n",
       " 'co': 242,\n",
       " 'down': 243,\n",
       " 'sms': 244,\n",
       " 'dun': 245,\n",
       " 'friends': 246,\n",
       " 'v': 247,\n",
       " \"that's\": 248,\n",
       " 'gud': 249,\n",
       " 'other': 250,\n",
       " 'wan': 251,\n",
       " 'help': 252,\n",
       " 'x': 253,\n",
       " 'things': 254,\n",
       " 'told': 255,\n",
       " 'wish': 256,\n",
       " 'hello': 257,\n",
       " 'waiting': 258,\n",
       " '16': 259,\n",
       " 'ã\\x8cã\\x8f': 260,\n",
       " 'fine': 261,\n",
       " 'special': 262,\n",
       " '18': 263,\n",
       " \"you're\": 264,\n",
       " 'haha': 265,\n",
       " 'coming': 266,\n",
       " 'may': 267,\n",
       " 'name': 268,\n",
       " 'getting': 269,\n",
       " 'done': 270,\n",
       " 'year': 271,\n",
       " 'same': 272,\n",
       " 'guaranteed': 273,\n",
       " 'yet': 274,\n",
       " 'people': 275,\n",
       " 'thk': 276,\n",
       " 'use': 277,\n",
       " 'try': 278,\n",
       " 'friend': 279,\n",
       " 'mins': 280,\n",
       " 'heart': 281,\n",
       " 'thought': 282,\n",
       " '6': 283,\n",
       " 'holiday': 284,\n",
       " 'lunch': 285,\n",
       " 'live': 286,\n",
       " 'man': 287,\n",
       " 'best': 288,\n",
       " 'talk': 289,\n",
       " 'stuff': 290,\n",
       " 'class': 291,\n",
       " 'y': 292,\n",
       " 'smile': 293,\n",
       " \"didn't\": 294,\n",
       " 'draw': 295,\n",
       " 'few': 296,\n",
       " 'cs': 297,\n",
       " 'days': 298,\n",
       " '7': 299,\n",
       " 'being': 300,\n",
       " 'yup': 301,\n",
       " 'trying': 302,\n",
       " 'bit': 303,\n",
       " 'never': 304,\n",
       " 'meeting': 305,\n",
       " 'thats': 306,\n",
       " 'job': 307,\n",
       " 'better': 308,\n",
       " 'house': 309,\n",
       " 'line': 310,\n",
       " 'finish': 311,\n",
       " 'cool': 312,\n",
       " 'long': 313,\n",
       " 'ill': 314,\n",
       " 'ready': 315,\n",
       " 'person': 316,\n",
       " 'having': 317,\n",
       " 'car': 318,\n",
       " 'mind': 319,\n",
       " 'end': 320,\n",
       " 'enjoy': 321,\n",
       " 'ã¥â£1': 322,\n",
       " 'latest': 323,\n",
       " 'half': 324,\n",
       " 'play': 325,\n",
       " 'check': 326,\n",
       " 'real': 327,\n",
       " 'yo': 328,\n",
       " 'wk': 329,\n",
       " 'account': 330,\n",
       " 'because': 331,\n",
       " 'dat': 332,\n",
       " 'than': 333,\n",
       " 'chance': 334,\n",
       " 'god': 335,\n",
       " 'lar': 336,\n",
       " 'receive': 337,\n",
       " 'word': 338,\n",
       " 'camera': 339,\n",
       " 'eat': 340,\n",
       " 'awarded': 341,\n",
       " 'wanna': 342,\n",
       " 'nothing': 343,\n",
       " 'guess': 344,\n",
       " 'lot': 345,\n",
       " 'sir': 346,\n",
       " 'problem': 347,\n",
       " '1st': 348,\n",
       " 'world': 349,\n",
       " 'another': 350,\n",
       " 'liao': 351,\n",
       " 'big': 352,\n",
       " 'dinner': 353,\n",
       " 'month': 354,\n",
       " 'ah': 355,\n",
       " 'birthday': 356,\n",
       " 'shows': 357,\n",
       " 'guys': 358,\n",
       " 'start': 359,\n",
       " 'into': 360,\n",
       " 'shit': 361,\n",
       " 'sweet': 362,\n",
       " 'ã¥â£1000': 363,\n",
       " 'girl': 364,\n",
       " 'luv': 365,\n",
       " 'jus': 366,\n",
       " 'might': 367,\n",
       " 'box': 368,\n",
       " 'ever': 369,\n",
       " 'quite': 370,\n",
       " 'cost': 371,\n",
       " 'watching': 372,\n",
       " 'room': 373,\n",
       " '150ppm': 374,\n",
       " 'landline': 375,\n",
       " 'bt': 376,\n",
       " 'offer': 377,\n",
       " 'video': 378,\n",
       " 'early': 379,\n",
       " 'xxx': 380,\n",
       " 'speak': 381,\n",
       " 'once': 382,\n",
       " 'aight': 383,\n",
       " 'tv': 384,\n",
       " 'called': 385,\n",
       " 'watch': 386,\n",
       " 'probably': 387,\n",
       " 'rate': 388,\n",
       " 'apply': 389,\n",
       " 'wont': 390,\n",
       " 'remember': 391,\n",
       " 'does': 392,\n",
       " 'maybe': 393,\n",
       " 'hear': 394,\n",
       " 'pa': 395,\n",
       " 'bed': 396,\n",
       " 'forgot': 397,\n",
       " 'll': 398,\n",
       " 'boy': 399,\n",
       " 'po': 400,\n",
       " 'thanx': 401,\n",
       " 'plan': 402,\n",
       " 'shall': 403,\n",
       " 'two': 404,\n",
       " 'minutes': 405,\n",
       " 'sat': 406,\n",
       " 'actually': 407,\n",
       " 'den': 408,\n",
       " 'bad': 409,\n",
       " 'princess': 410,\n",
       " 'fun': 411,\n",
       " '9': 412,\n",
       " 'code': 413,\n",
       " 'pay': 414,\n",
       " 'left': 415,\n",
       " 'ringtone': 416,\n",
       " 'look': 417,\n",
       " 'weekend': 418,\n",
       " 'part': 419,\n",
       " 'between': 420,\n",
       " 'easy': 421,\n",
       " 'reach': 422,\n",
       " 'shopping': 423,\n",
       " 'baby': 424,\n",
       " 'dunno': 425,\n",
       " 'orange': 426,\n",
       " 'office': 427,\n",
       " 'kiss': 428,\n",
       " '2nd': 429,\n",
       " \"he's\": 430,\n",
       " 'dis': 431,\n",
       " '10': 432,\n",
       " 'little': 433,\n",
       " 'leh': 434,\n",
       " 'face': 435,\n",
       " 'didnt': 436,\n",
       " 'hour': 437,\n",
       " 'network': 438,\n",
       " 'selected': 439,\n",
       " 'enough': 440,\n",
       " '000': 441,\n",
       " 'thank': 442,\n",
       " 'bus': 443,\n",
       " \"how's\": 444,\n",
       " 'looking': 445,\n",
       " 'anyway': 446,\n",
       " 'award': 447,\n",
       " 'those': 448,\n",
       " 'm': 449,\n",
       " 'working': 450,\n",
       " 'everything': 451,\n",
       " 'made': 452,\n",
       " 'put': 453,\n",
       " 'wife': 454,\n",
       " 'most': 455,\n",
       " 'afternoon': 456,\n",
       " 'without': 457,\n",
       " 'missing': 458,\n",
       " 'tmr': 459,\n",
       " 'evening': 460,\n",
       " 'collect': 461,\n",
       " 'asked': 462,\n",
       " 'texts': 463,\n",
       " '8': 464,\n",
       " 'while': 465,\n",
       " 'fuck': 466,\n",
       " 'dad': 467,\n",
       " 'town': 468,\n",
       " 'until': 469,\n",
       " 'wif': 470,\n",
       " 'though': 471,\n",
       " \"there's\": 472,\n",
       " 'calls': 473,\n",
       " 'since': 474,\n",
       " 'came': 475,\n",
       " 'okay': 476,\n",
       " 'says': 477,\n",
       " 'must': 478,\n",
       " 'school': 479,\n",
       " 'join': 480,\n",
       " 'mail': 481,\n",
       " 'sexy': 482,\n",
       " 'xmas': 483,\n",
       " 'true': 484,\n",
       " 'details': 485,\n",
       " 'entry': 486,\n",
       " 'goes': 487,\n",
       " 'update': 488,\n",
       " 'wanted': 489,\n",
       " 'pain': 490,\n",
       " 'means': 491,\n",
       " 'abt': 492,\n",
       " 'able': 493,\n",
       " 'hav': 494,\n",
       " 'important': 495,\n",
       " 'g': 496,\n",
       " 'wake': 497,\n",
       " 'tones': 498,\n",
       " 'wot': 499,\n",
       " 'bring': 500,\n",
       " 'collection': 501,\n",
       " 'times': 502,\n",
       " 'messages': 503,\n",
       " 'missed': 504,\n",
       " 'mob': 505,\n",
       " 'show': 506,\n",
       " 'price': 507,\n",
       " 'juz': 508,\n",
       " 'years': 509,\n",
       " 'decimal': 510,\n",
       " 'plz': 511,\n",
       " 'de': 512,\n",
       " 'away': 513,\n",
       " 'gift': 514,\n",
       " 'plus': 515,\n",
       " 'valid': 516,\n",
       " 'ã¥â£100': 517,\n",
       " 'alright': 518,\n",
       " 'till': 519,\n",
       " 're': 520,\n",
       " 'saw': 521,\n",
       " 'yesterday': 522,\n",
       " 'hair': 523,\n",
       " 'wen': 524,\n",
       " 'havent': 525,\n",
       " 'else': 526,\n",
       " 'worry': 527,\n",
       " '500': 528,\n",
       " '10p': 529,\n",
       " 'music': 530,\n",
       " 'weekly': 531,\n",
       " 'attempt': 532,\n",
       " 'guy': 533,\n",
       " 'colour': 534,\n",
       " 'net': 535,\n",
       " 'words': 536,\n",
       " 'yours': 537,\n",
       " 'double': 538,\n",
       " 'run': 539,\n",
       " 'making': 540,\n",
       " 'food': 541,\n",
       " 'haf': 542,\n",
       " 'til': 543,\n",
       " 'id': 544,\n",
       " 'oso': 545,\n",
       " 'shop': 546,\n",
       " 'book': 547,\n",
       " 'dude': 548,\n",
       " 'stay': 549,\n",
       " 'bored': 550,\n",
       " 'online': 551,\n",
       " 'makes': 552,\n",
       " 'lei': 553,\n",
       " 'question': 554,\n",
       " 'national': 555,\n",
       " 'ard': 556,\n",
       " \"we're\": 557,\n",
       " \"won't\": 558,\n",
       " 'tried': 559,\n",
       " 'delivery': 560,\n",
       " 'yourself': 561,\n",
       " \"haven't\": 562,\n",
       " 'driving': 563,\n",
       " 'test': 564,\n",
       " 'address': 565,\n",
       " 'answer': 566,\n",
       " 'top': 567,\n",
       " 'coz': 568,\n",
       " \"what's\": 569,\n",
       " 'nite': 570,\n",
       " 'hot': 571,\n",
       " 'hurt': 572,\n",
       " 'friendship': 573,\n",
       " 'change': 574,\n",
       " 'feeling': 575,\n",
       " 'either': 576,\n",
       " 'these': 577,\n",
       " 'sch': 578,\n",
       " 'family': 579,\n",
       " 'goin': 580,\n",
       " 'hours': 581,\n",
       " 'date': 582,\n",
       " 'http': 583,\n",
       " 'bonus': 584,\n",
       " 'trip': 585,\n",
       " 'comes': 586,\n",
       " 'ã¥â£5000': 587,\n",
       " 'movie': 588,\n",
       " 'busy': 589,\n",
       " \"''\": 590,\n",
       " 'todays': 591,\n",
       " 'order': 592,\n",
       " 'believe': 593,\n",
       " 'both': 594,\n",
       " 'vouchers': 595,\n",
       " 'wid': 596,\n",
       " 'full': 597,\n",
       " 'calling': 598,\n",
       " 'tot': 599,\n",
       " 'beautiful': 600,\n",
       " 'sae': 601,\n",
       " 'lose': 602,\n",
       " 'game': 603,\n",
       " 'together': 604,\n",
       " 'wants': 605,\n",
       " '8007': 606,\n",
       " 'sad': 607,\n",
       " 'set': 608,\n",
       " 'smiling': 609,\n",
       " 'mean': 610,\n",
       " 'old': 611,\n",
       " 'points': 612,\n",
       " 'ã¥â£2000': 613,\n",
       " 'leaving': 614,\n",
       " 'story': 615,\n",
       " 'sleeping': 616,\n",
       " 'noe': 617,\n",
       " 'happen': 618,\n",
       " 'ring': 619,\n",
       " 'club': 620,\n",
       " 'charge': 621,\n",
       " 'games': 622,\n",
       " \"we'll\": 623,\n",
       " 'chikku': 624,\n",
       " 'huh': 625,\n",
       " 'eve': 626,\n",
       " 'ã¥â£500': 627,\n",
       " 'saying': 628,\n",
       " 'drive': 629,\n",
       " 'await': 630,\n",
       " 'dreams': 631,\n",
       " 'brother': 632,\n",
       " 'pounds': 633,\n",
       " 'news': 634,\n",
       " 'aft': 635,\n",
       " 'tomo': 636,\n",
       " 'congrats': 637,\n",
       " 'took': 638,\n",
       " 'finished': 639,\n",
       " 'started': 640,\n",
       " 'private': 641,\n",
       " 'gr8': 642,\n",
       " 'awesome': 643,\n",
       " 'minute': 644,\n",
       " 'walk': 645,\n",
       " '86688': 646,\n",
       " 'okie': 647,\n",
       " 'post': 648,\n",
       " 'row': 649,\n",
       " 'poly': 650,\n",
       " 'pm': 651,\n",
       " 'thinking': 652,\n",
       " 'pics': 653,\n",
       " 'email': 654,\n",
       " 'rite': 655,\n",
       " 'pic': 656,\n",
       " 'available': 657,\n",
       " 'final': 658,\n",
       " \"c's\": 659,\n",
       " 'tho': 660,\n",
       " 'forget': 661,\n",
       " 'second': 662,\n",
       " 'close': 663,\n",
       " 'cause': 664,\n",
       " 'services': 665,\n",
       " 'taking': 666,\n",
       " 'everyone': 667,\n",
       " 'wil': 668,\n",
       " 'angry': 669,\n",
       " '750': 670,\n",
       " 'unsubscribe': 671,\n",
       " 'lets': 672,\n",
       " 'drink': 673,\n",
       " 'head': 674,\n",
       " 'land': 675,\n",
       " 'gd': 676,\n",
       " 'neva': 677,\n",
       " 'pub': 678,\n",
       " \"she's\": 679,\n",
       " 'drop': 680,\n",
       " 'auction': 681,\n",
       " '11': 682,\n",
       " 'lesson': 683,\n",
       " 'lucky': 684,\n",
       " 'xx': 685,\n",
       " 'search': 686,\n",
       " '12hrs': 687,\n",
       " 'statement': 688,\n",
       " 'expires': 689,\n",
       " 'msgs': 690,\n",
       " 'open': 691,\n",
       " 'whats': 692,\n",
       " 'lots': 693,\n",
       " 'each': 694,\n",
       " 'smoke': 695,\n",
       " 'worth': 696,\n",
       " 'sis': 697,\n",
       " 'touch': 698,\n",
       " 'found': 699,\n",
       " 'break': 700,\n",
       " 'sounds': 701,\n",
       " 'company': 702,\n",
       " 'choose': 703,\n",
       " 'card': 704,\n",
       " 'w': 705,\n",
       " 'sister': 706,\n",
       " 'dating': 707,\n",
       " 'opt': 708,\n",
       " 'simple': 709,\n",
       " 'mine': 710,\n",
       " 'whatever': 711,\n",
       " 'voucher': 712,\n",
       " 'knw': 713,\n",
       " 'anyone': 714,\n",
       " 'don': 715,\n",
       " 'loving': 716,\n",
       " 'alone': 717,\n",
       " 'treat': 718,\n",
       " 'winner': 719,\n",
       " '100': 720,\n",
       " 'info': 721,\n",
       " 'pobox': 722,\n",
       " 'ha': 723,\n",
       " 'smth': 724,\n",
       " 'saturday': 725,\n",
       " 'decided': 726,\n",
       " '08000930705': 727,\n",
       " 'girls': 728,\n",
       " 'prob': 729,\n",
       " 'gone': 730,\n",
       " 'happened': 731,\n",
       " 'identifier': 732,\n",
       " 'nt': 733,\n",
       " 'type': 734,\n",
       " 'ltd': 735,\n",
       " 'hard': 736,\n",
       " 'frnd': 737,\n",
       " 'needs': 738,\n",
       " 'carlos': 739,\n",
       " 'boytoy': 740,\n",
       " 'college': 741,\n",
       " 'takes': 742,\n",
       " 'anytime': 743,\n",
       " 'far': 744,\n",
       " 'mobileupd8': 745,\n",
       " 'kind': 746,\n",
       " 'visit': 747,\n",
       " 'fast': 748,\n",
       " 'mum': 749,\n",
       " 'sun': 750,\n",
       " 'crazy': 751,\n",
       " 'wonderful': 752,\n",
       " \"doesn't\": 753,\n",
       " 'camcorder': 754,\n",
       " 'used': 755,\n",
       " 'hit': 756,\n",
       " 'operator': 757,\n",
       " 'friday': 758,\n",
       " 'quiz': 759,\n",
       " 'player': 760,\n",
       " 'parents': 761,\n",
       " 'hand': 762,\n",
       " 'content': 763,\n",
       " 'wit': 764,\n",
       " \"you've\": 765,\n",
       " 'finally': 766,\n",
       " 'darlin': 767,\n",
       " 'rs': 768,\n",
       " 'goodmorning': 769,\n",
       " 'oredi': 770,\n",
       " 'secret': 771,\n",
       " 'tel': 772,\n",
       " 'congratulations': 773,\n",
       " 'read': 774,\n",
       " 'light': 775,\n",
       " 'suite342': 776,\n",
       " '2lands': 777,\n",
       " '08000839402': 778,\n",
       " 'bout': 779,\n",
       " 'fucking': 780,\n",
       " 'nope': 781,\n",
       " 'outside': 782,\n",
       " 'fri': 783,\n",
       " 'ã¥â£3': 784,\n",
       " 'pretty': 785,\n",
       " 'sea': 786,\n",
       " 'o': 787,\n",
       " 'weeks': 788,\n",
       " 'â\\x80°ã\\x9b': 789,\n",
       " 'lovely': 790,\n",
       " 'mates': 791,\n",
       " 'wrong': 792,\n",
       " 'chennai': 793,\n",
       " 'hows': 794,\n",
       " '30': 795,\n",
       " 'wkly': 796,\n",
       " 'freemsg': 797,\n",
       " \"'\": 798,\n",
       " 'sunday': 799,\n",
       " 'credit': 800,\n",
       " 'hungry': 801,\n",
       " 'seeing': 802,\n",
       " 'telling': 803,\n",
       " 'whole': 804,\n",
       " 'frnds': 805,\n",
       " 'hmm': 806,\n",
       " 'mu': 807,\n",
       " \"you'll\": 808,\n",
       " 'yr': 809,\n",
       " 'their': 810,\n",
       " 'ni8': 811,\n",
       " 'f': 812,\n",
       " 'fancy': 813,\n",
       " 'bank': 814,\n",
       " 'log': 815,\n",
       " 'course': 816,\n",
       " 'tc': 817,\n",
       " 'thinks': 818,\n",
       " 'case': 819,\n",
       " 'meant': 820,\n",
       " 'hold': 821,\n",
       " 'unlimited': 822,\n",
       " 'blue': 823,\n",
       " 'fone': 824,\n",
       " 'project': 825,\n",
       " 'reason': 826,\n",
       " 'ã¥â£250': 827,\n",
       " 'ten': 828,\n",
       " 'welcome': 829,\n",
       " 'cum': 830,\n",
       " 'frm': 831,\n",
       " 'savamob': 832,\n",
       " 'offers': 833,\n",
       " 'listen': 834,\n",
       " 'snow': 835,\n",
       " 'b4': 836,\n",
       " 'mate': 837,\n",
       " 'least': 838,\n",
       " 'earlier': 839,\n",
       " 'party': 840,\n",
       " 'point': 841,\n",
       " 'press': 842,\n",
       " 'valued': 843,\n",
       " 'almost': 844,\n",
       " 'etc': 845,\n",
       " 'cut': 846,\n",
       " 'hee': 847,\n",
       " 'download': 848,\n",
       " '0800': 849,\n",
       " 'mah': 850,\n",
       " 'felt': 851,\n",
       " 'caller': 852,\n",
       " '03': 853,\n",
       " 'numbers': 854,\n",
       " 'age': 855,\n",
       " 'tired': 856,\n",
       " 'hmmm': 857,\n",
       " 'mr': 858,\n",
       " 'mrng': 859,\n",
       " 'balance': 860,\n",
       " 'march': 861,\n",
       " 'side': 862,\n",
       " 'fr': 863,\n",
       " '87066': 864,\n",
       " 'dnt': 865,\n",
       " 'stupid': 866,\n",
       " 'bslvyl': 867,\n",
       " 'lost': 868,\n",
       " 'christmas': 869,\n",
       " 'reading': 870,\n",
       " 'txts': 871,\n",
       " 'ago': 872,\n",
       " 'currently': 873,\n",
       " 'motorola': 874,\n",
       " 'talking': 875,\n",
       " 'couple': 876,\n",
       " 'phones': 877,\n",
       " 'ass': 878,\n",
       " 'india': 879,\n",
       " 'park': 880,\n",
       " 'ã¥â£2': 881,\n",
       " 'within': 882,\n",
       " '2003': 883,\n",
       " '800': 884,\n",
       " 'un': 885,\n",
       " 'yar': 886,\n",
       " 'happiness': 887,\n",
       " 'area': 888,\n",
       " 'ã¥â£350': 889,\n",
       " 'sex': 890,\n",
       " 'mayb': 891,\n",
       " 'understand': 892,\n",
       " 'support': 893,\n",
       " 'na': 894,\n",
       " 'luck': 895,\n",
       " 'enter': 896,\n",
       " 'gas': 897,\n",
       " 'father': 898,\n",
       " 'comp': 899,\n",
       " \"i'd\": 900,\n",
       " 'mobiles': 901,\n",
       " '20': 902,\n",
       " 'eh': 903,\n",
       " 'charged': 904,\n",
       " 'confirm': 905,\n",
       " 'wow': 906,\n",
       " 'ac': 907,\n",
       " 'red': 908,\n",
       " 'correct': 909,\n",
       " 'pass': 910,\n",
       " 'song': 911,\n",
       " 'complimentary': 912,\n",
       " 'gotta': 913,\n",
       " 'computer': 914,\n",
       " 'mom': 915,\n",
       " 'askd': 916,\n",
       " 'invited': 917,\n",
       " 'uncle': 918,\n",
       " 'sending': 919,\n",
       " 'direct': 920,\n",
       " 'semester': 921,\n",
       " 'reveal': 922,\n",
       " 'laptop': 923,\n",
       " 'questions': 924,\n",
       " 'swing': 925,\n",
       " 'ends': 926,\n",
       " 'die': 927,\n",
       " 'via': 928,\n",
       " 'met': 929,\n",
       " 'st': 930,\n",
       " 'call2optout': 931,\n",
       " 'seen': 932,\n",
       " 'rental': 933,\n",
       " 'th': 934,\n",
       " 'supposed': 935,\n",
       " 'ipod': 936,\n",
       " 'redeemed': 937,\n",
       " '04': 938,\n",
       " 'through': 939,\n",
       " 'gym': 940,\n",
       " 'darren': 941,\n",
       " 'ans': 942,\n",
       " 'picking': 943,\n",
       " 'ugh': 944,\n",
       " 'extra': 945,\n",
       " 'knew': 946,\n",
       " 'heard': 947,\n",
       " 'information': 948,\n",
       " 'surprise': 949,\n",
       " 'grins': 950,\n",
       " 'gal': 951,\n",
       " 'difficult': 952,\n",
       " 'john': 953,\n",
       " \"wasn't\": 954,\n",
       " 'std': 955,\n",
       " 'usf': 956,\n",
       " 'reward': 957,\n",
       " '12': 958,\n",
       " 'wap': 959,\n",
       " 'eg': 960,\n",
       " 'comin': 961,\n",
       " 'abiola': 962,\n",
       " 'crave': 963,\n",
       " 'gets': 964,\n",
       " 'move': 965,\n",
       " 'checking': 966,\n",
       " 'rply': 967,\n",
       " 'loads': 968,\n",
       " 'shower': 969,\n",
       " \"isn't\": 970,\n",
       " 'entered': 971,\n",
       " 'match': 972,\n",
       " 'dogging': 973,\n",
       " 'txting': 974,\n",
       " 'lovable': 975,\n",
       " 'wine': 976,\n",
       " 'dream': 977,\n",
       " 'safe': 978,\n",
       " 'muz': 979,\n",
       " 'bath': 980,\n",
       " 'orchard': 981,\n",
       " 'kate': 982,\n",
       " 'exam': 983,\n",
       " 'bcoz': 984,\n",
       " 'own': 985,\n",
       " 'wana': 986,\n",
       " 'somebody': 987,\n",
       " 'rest': 988,\n",
       " 'plans': 989,\n",
       " 'small': 990,\n",
       " 'jay': 991,\n",
       " 'ex': 992,\n",
       " 'hg': 993,\n",
       " 'w1j6hl': 994,\n",
       " 'discount': 995,\n",
       " 'slow': 996,\n",
       " 'rock': 997,\n",
       " 'asking': 998,\n",
       " 'remove': 999,\n",
       " 'monday': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 507)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text into sequence of tokens\n",
    "#also pad them in order to convert to equal vector length\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data[\"text\"],target)\n",
    "trainx = sequence.pad_sequences(token.texts_to_sequences(X_train),maxlen=70)\n",
    "valx = sequence.pad_sequences(token.texts_to_sequences(X_test),maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-b2cf20528f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#to fill this emdedding matrix we need to iterate word by word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0memdedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_index' is not defined"
     ]
    }
   ],
   "source": [
    "#generate sequence of tokens as iput and them to length 70\n",
    "#create an embedding matrix - embeding vectors for keywords present in every text\n",
    "#maxlengh is 300 in the pretrained model provided\n",
    "#no of rows as documents, no of columns as no of words\n",
    "#individivaul word emebedding where\n",
    "#every row corresponds to the word and column represents vector notation\n",
    "embedding_matrix = np.zeros((len(word_index)+1,300))\n",
    "\n",
    "#to fill this emdedding matrix we need to iterate word by word\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if emdedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this case we will write a function to train the model\n",
    "def train_model(classifier,feature_vector_train,label,feature_vector_val,valid_y):\n",
    "    classifier.fit(feature_vector_train,label)\n",
    "    predictions = classifier.predict(feature_vector_val)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return accurcay_score(predictions,valid_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models,optimizers\n",
    "def create_cnn():\n",
    "    #input_size = 70\n",
    "    input_layer = layers.input((70,))\n",
    "    embedded_layer = layers.Embedding(len(word_index)+1,300,weights=[embedding_matrix],trainable=False)(input_layer)\n",
    "    conv_layer = layers.Convolution1D(100,3,activation=\"relu\")(embedding_layer)\n",
    "    pooling_layer =  layers.GlobalMaxpool1D(conv_layer)\n",
    "    output_layer = layers.Dense(50,activation=\"relu\")(pooling_layer)\n",
    "    output_layer = layers.Dropout(0.25)(pooling_layer)\n",
    "    output_layer = layers.Dense(1,activation=\"sigmoid\")(output_layer)\n",
    "    model = models.Model(Inputs - Input_layer,outputs = output_layer)\n",
    "    model.compile(optimizer=optimizers.Adam(),loss=\"binary_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = create_cnn()\n",
    "train_model(classifier,trainx,trainy,valx,valy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more convolutional layers can be added in order to improve performance.\n",
    "#Dropouts can be experimented with different values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
