{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German to English Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "#\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some observations I note from reviewing the raw data:\n",
    "\n",
    "* There is punctuation.\n",
    "\n",
    "* The text contains uppercase and lowercase.\n",
    "\n",
    "* There are special characters in the German.\n",
    "\n",
    "* There are duplicate phrases in English with different translations in German.\n",
    "\n",
    "* The file is ordered by sentence length with very long sentences toward the end of the fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is divided into two subsections:\n",
    "\n",
    " - Clean Text\n",
    "\n",
    " - Split Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data in a way that preserves the Unicode German characters. \n",
    "#The function below called load_doc() will load the file as a blob of text.\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line contains a single pair of phrases, first English and then German, separated by a tab character.\n",
    "\n",
    "We must split the loaded text by line and then by phrase. \n",
    "\n",
    "The function to_pairs() below will split the loaded text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences with pairs having English and corresponing Germain word pair\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean each sentence. \n",
    "\n",
    "The specific cleaning operations we will perform are as follows:\n",
    "\n",
    "- Remove all non-printable characters.\n",
    "\n",
    "- Remove all punctuation characters.\n",
    "\n",
    "- Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "\n",
    "- Normalize the case to lowercase.\n",
    "\n",
    "- Remove any remaining tokens that are not alphabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pair(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Hi.', 'Hallo!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[hi] => [hallo]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Hi.', 'Grüß Gott!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[hi] => [gru gott]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Run!', 'Lauf!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[run] => [lauf]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Wow!', 'Potzdonner!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[wow] => [potzdonner]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Wow!', 'Donnerwetter!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[wow] => [donnerwetter]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Fire!', 'Feuer!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[fire] => [feuer]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Help!', 'Hilfe!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[help] => [hilfe]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Help!', 'Zu Hülf!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[help] => [zu hulf]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Stop!', 'Stopp!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[stop] => [stopp]\n",
      "Unclean \n",
      "\n",
      "==============================\n",
      "['Wait!', 'Warte!']\n",
      "clean \n",
      "\n",
      "==============================\n",
      "[wait] => [warte]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'deu.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_pair(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(\"Unclean \\n\")\n",
    "    print(\"=\"*30)\n",
    "    print(pairs[i])\n",
    "    print(\"clean \\n\")\n",
    "    print(\"=\"*30)\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\\!\\\"\\#\\$\\%\\&\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~\\ \\\\t\\\\n\\\\r\\\\x0b\\\\x0c]',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "re_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates a dictionary with initial values initialized as None for each punctuation symbol\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean data contains a little over 150,000 phrase pairs and some of the pairs toward the end of the file are very long.\n",
    "\n",
    "We will simplify the problem by reducing the dataset to the first 10,000 examples in the file; these will be the shortest phrases in the dataset.\n",
    "\n",
    "Further, we will then stake the first 9,000 of those as examples for training and the remaining 1,000 examples to test the fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates three new files: the english-german-both.pkl that contains all of the train and test examples that we can use to define the parameters of the problem, such as max phrase lengths and the vocabulary, and the english-german-train.pkl and english-german-test.pkl files for the train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves both loading and preparing the clean text data ready for modeling and defining and training the model on the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use the Keras Tokenize class to map words to integers, as needed for modeling. \n",
    "\n",
    "\n",
    "We will use separate tokenizer for the English sequences and the German sequences. \n",
    "\n",
    "The function below-named create_tokenizer() will train a tokenizer on a list of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the function named max_length() below will find the length of the longest sequence in a list of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['get out', 'raus'],\n",
       "       ['i want to play', 'ich will spielen'],\n",
       "       ['you can rest', 'sie konnen sich ausruhen'],\n",
       "       ['he relented', 'er gab nach'],\n",
       "       ['its not enough', 'das reicht nicht aus']], dtype='<U370')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare tokenizers, vocabulary sizes, and maximum lengths for both the English and German phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2309\n",
      "English Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 3657\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "#word index returns  a dictionary of the word and its index position\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each input and output sequence must be encoded to integers and padded to the maximum phrase length. \n",
    "\n",
    "- This is because we will use a word embedding for the input sequences and one hot encode the output sequences \n",
    "\n",
    "The function below named encode_sequences() will perform these operations and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t#print((lines,X))\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output sequence needs to be one-hot encoded. This is because the model will predict the probability of each word in the vocabulary as output.\n",
    "\n",
    "The function encode_output() below will one-hot encode English output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  prepare both the train and test dataset ready for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['get out', 'i want to play', 'you can rest', 'he relented',\n",
       "       'its not enough', 'it has to stop', 'come back', 'i like to sing',\n",
       "       'go away', 'is tom reliable'], dtype='<U370')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wir stehen', 'ein mensch muss arbeiten', 'tom ist nicht verloren',\n",
       "       'sie horten auf', 'macht eure taschen leer', 'er ist mein vater',\n",
       "       'schauen sie genau hin', 'fasst mit an', 'nimm das',\n",
       "       'ich hatte unrecht'], dtype='<U370')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use an encoder-decoder LSTM model on this problem. \n",
    "\n",
    "In this architecture, the input sequence is encoded by a front-end model called the encoder then decoded word by word by a backend model called the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function define_model() below defines the model and takes a number of arguments used to configure the model, such as \n",
    "\n",
    "- the size of the input and output vocabularies, \n",
    "\n",
    "- the maximum length of input and output phrases, and \n",
    "\n",
    "- the number of memory units used to configure the model.\n",
    "\n",
    "\n",
    "The model is trained using the efficient Adam approach to stochastic gradient descent and minimizes the categorical loss function because we have framed the prediction problem as multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in d:\\users\\plaban_nayak\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.10.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fbprophet 0.4.post2 requires setuptools-git>=1.2, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 10, 256)           936192    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_9 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 5, 2309)           593413    \n",
      "=================================================================\n",
      "Total params: 2,580,229\n",
      "Trainable params: 2,580,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the model.\n",
    "\n",
    "We train the model for 30 epochs and a batch size of 64 examples.\n",
    "\n",
    "We use checkpointing to ensure that each time the model skill on the test set improves, the model is saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 47s - loss: 4.3278 - val_loss: 3.5656\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.56561, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 32s - loss: 3.4131 - val_loss: 3.4259\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.56561 to 3.42591, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 33s - loss: 3.2560 - val_loss: 3.3179\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.42591 to 3.31793, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 34s - loss: 3.1018 - val_loss: 3.2021\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.31793 to 3.20206, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 34s - loss: 2.9721 - val_loss: 3.1067\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.20206 to 3.10666, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 34s - loss: 2.8132 - val_loss: 2.9849\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.10666 to 2.98485, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 33s - loss: 2.6501 - val_loss: 2.8713\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.98485 to 2.87132, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 33s - loss: 2.4915 - val_loss: 2.7667\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.87132 to 2.76671, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 32s - loss: 2.3292 - val_loss: 2.6686\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.76671 to 2.66858, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 32s - loss: 2.1819 - val_loss: 2.5776\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.66858 to 2.57759, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 32s - loss: 2.0409 - val_loss: 2.5061\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.57759 to 2.50610, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 32s - loss: 1.9086 - val_loss: 2.4315\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.50610 to 2.43153, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 33s - loss: 1.7840 - val_loss: 2.3768\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.43153 to 2.37676, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 31s - loss: 1.6680 - val_loss: 2.3165\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.37676 to 2.31647, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 34s - loss: 1.5554 - val_loss: 2.2707\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.31647 to 2.27074, saving model to model.h5\n",
      "Epoch 16/30\n",
      " - 33s - loss: 1.4490 - val_loss: 2.2154\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.27074 to 2.21537, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 33s - loss: 1.3419 - val_loss: 2.1886\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.21537 to 2.18861, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 32s - loss: 1.2444 - val_loss: 2.1598\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.18861 to 2.15980, saving model to model.h5\n",
      "Epoch 19/30\n",
      " - 34s - loss: 1.1516 - val_loss: 2.1288\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.15980 to 2.12881, saving model to model.h5\n",
      "Epoch 20/30\n",
      " - 35s - loss: 1.0681 - val_loss: 2.0886\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.12881 to 2.08859, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 42s - loss: 0.9802 - val_loss: 2.0753\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.08859 to 2.07532, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 45s - loss: 0.9027 - val_loss: 2.0455\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.07532 to 2.04551, saving model to model.h5\n",
      "Epoch 23/30\n",
      " - 34s - loss: 0.8285 - val_loss: 2.0376\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.04551 to 2.03763, saving model to model.h5\n",
      "Epoch 24/30\n",
      " - 35s - loss: 0.7591 - val_loss: 2.0192\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.03763 to 2.01924, saving model to model.h5\n",
      "Epoch 25/30\n",
      " - 40s - loss: 0.6966 - val_loss: 2.0085\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.01924 to 2.00850, saving model to model.h5\n",
      "Epoch 26/30\n",
      " - 37s - loss: 0.6365 - val_loss: 1.9912\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.00850 to 1.99121, saving model to model.h5\n",
      "Epoch 27/30\n",
      " - 36s - loss: 0.5832 - val_loss: 1.9960\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.99121\n",
      "Epoch 28/30\n",
      " - 33s - loss: 0.5322 - val_loss: 1.9951\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.99121\n",
      "Epoch 29/30\n",
      " - 32s - loss: 0.4855 - val_loss: 1.9793\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.99121 to 1.97931, saving model to model.h5\n",
      "Epoch 30/30\n",
      " - 33s - loss: 0.4432 - val_loss: 1.9828\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.97931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5b8a76d8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the best model saved during training must be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation involves two steps: \n",
    "    \n",
    "    - first generating a translated output sequence, and \n",
    "    \n",
    "    - then repeating this process for many input examples and summarizing the skill of the model across multiple cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with inference, the model can predict the entire output sequence in a one-shot manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a sequence of integers that we can enumerate and lookup in the tokenizer to map back to words.\n",
    "\n",
    "The function below, named word_for_id(), will perform this reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform this mapping for each integer in the translation and return the result as a string of words.\n",
    "\n",
    "The function predict_sequence() below performs this operation for a single encoded source phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [np.argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can repeat this for each source phrase in a dataset and compare the predicted result to the expected target phrase in English.\n",
    "\n",
    "We can print some of these comparisons to screen to get an idea of how the model performs in practice.\n",
    "\n",
    "We will also calculate the BLEU scores to get a quantitative idea of how well the model has performed.\n",
    "\n",
    "The evaluate_model() function below implements this, calling the above predict_sequence() function for each phrase in a provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append(raw_target.split())\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[raus], target=[get out], predicted=[get out]\n",
      "src=[ich will spielen], target=[i want to play], predicted=[i want to raise]\n",
      "src=[sie konnen sich ausruhen], target=[you can rest], predicted=[you can rest]\n",
      "src=[er gab nach], target=[he relented], predicted=[he relented]\n",
      "src=[das reicht nicht aus], target=[its not enough], predicted=[its not enough]\n",
      "src=[es muss aufhoren], target=[it has to stop], predicted=[it has to stop]\n",
      "src=[komm wieder], target=[come back], predicted=[come back]\n",
      "src=[ich singe gern], target=[i like to sing], predicted=[i like to]\n",
      "src=[mach die sause], target=[go away], predicted=[get away]\n",
      "src=[ist tom zuverlassig], target=[is tom reliable], predicted=[is tom reliable]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Plaban_Nayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "D:\\Users\\Plaban_Nayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "D:\\Users\\Plaban_Nayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.077815\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "test\n",
      "src=[wir stehen], target=[were standing], predicted=[were are]\n",
      "src=[ein mensch muss arbeiten], target=[a man must work], predicted=[my one a fever]\n",
      "src=[tom ist nicht verloren], target=[tom isnt lost], predicted=[tom isnt]\n",
      "src=[sie horten auf], target=[they stopped], predicted=[they stood up]\n",
      "src=[macht eure taschen leer], target=[empty your bags], predicted=[close your bags]\n",
      "src=[er ist mein vater], target=[hes my father], predicted=[he is my dog]\n",
      "src=[schauen sie genau hin], target=[look closely], predicted=[watch closely]\n",
      "src=[fasst mit an], target=[lend us a hand], predicted=[look for]\n",
      "src=[nimm das], target=[take this], predicted=[take this]\n",
      "src=[ich hatte unrecht], target=[i was wrong], predicted=[i was wrong]\n",
      "BLEU-1: 0.081937\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "import numpy as np\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
