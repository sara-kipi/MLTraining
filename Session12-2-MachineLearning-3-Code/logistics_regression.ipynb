{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "Researchers are often interested in setting up a model to analyze the relationship between predictors (i.e., independent variables) and it's corresponsing response (i.e., dependent variable). Linear regression is commonly used when the response variable is continuous.  One assumption of linear models is that the residual errors follow a normal distribution. This assumption fails when the response variable is categorical, so an ordinary linear model is not appropriate. This newsletter presents a regression model for response variable that is dichotomous–having two categories. Examples are common: whether a plant lives or dies, whether a survey respondent agrees or disagrees with a statement, or whether an at-risk child graduates or drops out from high school.\n",
    "\n",
    "In ordinary linear regression, the response variable (Y) is a linear function of the coefficients (B0, B1, etc.) that correspond to the predictor variables (X1, X2, etc.,). A typical model would look like:\n",
    "\n",
    "    Y = B0 + B1*X1 + B2*X2 + B3*X3 + … + E\n",
    "\n",
    "For a dichotomous response variable, we could set up a similar linear model to predict individual category memberships if numerical values are used to represent the two categories. Arbitrary values of 1 and 0 are chosen for mathematical convenience. Using the first example, we would assign Y = 1 if a plant lives and Y = 0 if a plant dies.\n",
    "\n",
    "This linear model does not work well for a few reasons. First, the response values, 0 and 1, are arbitrary, so modeling the actual values of Y is not exactly of interest. Second, it is the probability that each individual in the population responds with 0 or 1 that we are interested in modeling. For example, we may find that plants with a high level of a fungal infection (X1) fall into the category “the plant lives” (Y) less often than those plants with low level of infection. Thus, as the level of infection rises, the probability of plant living decreases.\n",
    "\n",
    "Thus, we might consider modeling P, the probability, as the response variable. Again, there are problems. Although the general decrease in probability is accompanied by a general increase in infection level, we know that P, like all probabilities, can only fall within the boundaries of 0 and 1. Consequently, it is better to assume that the relationship between X1 and P is sigmoidal (S-shaped), rather than a straight line.\n",
    "\n",
    "It is possible, however, to find a linear relationship between X1 and function of P. Although a number of functions work, one of the most useful is the logit function. It is the natural log of the odds that Y is equal to 1, which is simply the ratio of the probability that Y is 1 divided by the probability that Y is 0. The relationship between the logit of P and P itself is sigmoidal in shape. The regression equation that results is:\n",
    "\n",
    "    ln[P/(1-P)] = B0 + B1*X1 + B2*X2 + …\n",
    "\n",
    "Although the left side of this equation looks intimidating, this way of expressing the probability results in the right side of the equation being linear and looking familiar to us. This helps us understand the meaning of the regression coefficients. The coefficients can easily be transformed so that their interpretation makes sense.\n",
    "\n",
    "The logistic regression equation can be extended beyond the case of a dichotomous response variable to the cases of ordered categories and polytymous categories (more than two categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics behind Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem structure is the classic classification problem. Our data set $\\mathcal{D}$ is composed of $N$ samples. Each sample is a tuple containing a feature vector and a label. For any sample $n$ the feature vector is a $d+1$ dimensional column vector denoted by ${\\bf x}_n$ with $d$ real-valued components known as features. Samples are represented in homogeneous form with the first component equal to $1$: $x_0=1$. Vectors are bold-faced. The associated label is denoted $y_n$ and can take only two values: $+1$ or $-1$.\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\lbrace ({\\bf x}_1, y_1), ({\\bf x}_2, y_2), ..., ({\\bf x}_N, y_N) \\rbrace \\\\\n",
    "{\\bf x}_n = \\begin{bmatrix} 1 & x_1 & ... & x_d \\end{bmatrix}^T \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the name logistic *regression* this is actually a probabilistic classification model. It is also a linear model which can be subjected to nonlinear transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All linear models make use of a \"signal\" $s$ which is a linear combination of the input vector ${\\bf x}$ components weighed by the corresponding components in a weight vector ${\\bf w}$.\n",
    "\n",
    "$$\n",
    "{\\bf w} = \\begin{bmatrix} w_0 & w_1 & ... & w_d \\end{bmatrix}^T \\\\\n",
    "s = w_0 + w_1 x_1 + \\;...\\; + w_d x_d = \\sum_{i=0}^d w_i x_i = {\\bf w} \\cdot {\\bf x} = {\\bf w}^T {\\bf x}\n",
    "$$\n",
    "\n",
    "Note that the homogeneous representation (with the $1$ at the first component) allows us to include a constant offset using a more compact vector-only notation (instead of ${\\bf w}^T {\\bf x}+b$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classification passes the signal through a harsh threshold:\n",
    "\n",
    "$$\n",
    "h({\\bf x}) = \\operatorname{sign}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression uses the signal directly without modification:\n",
    "\n",
    "$$\n",
    "h({\\bf x}) = s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression passes the signal through the logistic/sigmoid but then treats the result as a probability:\n",
    "\n",
    "$$\n",
    "h({\\bf x}) = \\theta(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [logistic function](http://en.wikipedia.org/wiki/Logistic_function) is\n",
    "\n",
    "$$\n",
    "\\theta(s) = \\frac{e^s}{1+e^s} = \\frac{1}{1+e^{-s}}\n",
    "$$\n",
    "\n",
    "There are many other formulas that can achieve a soft threshold such as the hyperbolic tangent, but this function results in some nice simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that the data is generated by a noisy target.\n",
    "\n",
    "$$\n",
    "P(y\\mid{\\bf x})=\\begin{cases}\n",
    "f({\\bf x}) & \\text{for }y=+1 \\\\\n",
    "1-f({\\bf x}) & \\text{for }y=-1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this noisy target we want to learn a hypothesis $h({\\bf x})$ that best fits the above noisy target according to some error function.\n",
    "\n",
    "$$\n",
    "h({\\bf x})=\\theta({\\bf w}^T {\\bf x})\\approx f({\\bf x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the data does not tell you the probability of a label, rather it tells what label the sample has after being generated by the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn a good hypothesis we want to find a hypothesis parameterization ${\\bf w}$ (the weight vector) that minimizes some in-sample error measure $E_\\text{in}$.\n",
    "\n",
    "$$\n",
    "{\\bf w}_h = \\underset{{\\bf w}}{\\operatorname{argmin}} \\; E_\\text{in}({\\bf w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error measure will be used is both plausible and nice. It is based on likelihood which is the probability of generating the data given a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our hypothesis is close to our target distribution ($h\\approx f$) then we expect that probability of generating the data to be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some controversy with using likelihood. We are really looking for the most probable hypothesis given the data: $\\underset{h}{\\operatorname{argmax}} P(h\\mid{\\bf x})$. The likelihood approach is looking for the hypothesis that makes the data most probable: $\\underset{h}{\\operatorname{argmax}} P({\\bf x}\\mid h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian approach tackles this issue using Bayes' Theorem but introduces other issues such as choosing priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the likelihood we assume the data was generated with our hypothesis $h$:\n",
    "\n",
    "$$\n",
    "P(y\\mid{\\bf x})=\\begin{cases}\n",
    "h({\\bf x}) & \\text{for }y=+1 \\\\\n",
    "1-h({\\bf x}) & \\text{for }y=-1\n",
    "\\end{cases} \\\\\n",
    "$$\n",
    "\n",
    "where $h({\\bf x})=\\theta({\\bf w}^T {\\bf x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to deal with cases so we take advantage of a nice property of the logistic function: $\\theta(-s)=1-\\theta(s)$.\n",
    "\n",
    "$$\n",
    "\\text{if } y = +1 \\text{ then } h({\\bf x}) = \\theta({\\bf w}^T {\\bf x}) = \\theta(y \\; {\\bf w}^T {\\bf x}) \\\\\n",
    "\\text{if } y = -1 \\text{ then } 1 - h({\\bf x}) = 1 - \\theta({\\bf w}^T {\\bf x}) = \\theta(- {\\bf w}^T {\\bf x}) = \\theta(y \\; {\\bf w}^T {\\bf x}) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this simplification,\n",
    "\n",
    "$$\n",
    "P(y\\mid{\\bf x})=\\theta(y\\; {\\bf w}^T {\\bf x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is defined for a data set $\\mathcal{D}$ with $N$ samples given a hypothesis (denoted arbitrarily $g$ here):\n",
    "\n",
    "$$\n",
    "L(\\mathcal{D} \\mid g) =\n",
    "\\prod_{n=1}^{N} P(y_n \\mid {\\bf x}_n) =\n",
    "\\prod_{n=1}^{N} \\theta(y_n \\; {\\bf w}_g^T {\\bf x}_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finding a good hypothesis is a matter of finding a hypothesis parameterization ${\\bf w}$ that maximizes the likelihood.\n",
    "\n",
    "$$\n",
    "{\\bf w}_h =\n",
    "\\underset{{\\bf w}}{\\operatorname{argmax}} \\; L(\\mathcal{D} \\mid h) = \n",
    "\\underset{{\\bf w}}{\\operatorname{argmax}} \\; \\theta(y_n \\; {\\bf w}^T {\\bf x}_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing the likelihood is equivalent to maximizing the log of the function since the natural logarithm is a monotonically increasing function:\n",
    "\n",
    "$$\n",
    "\\underset{{\\bf w}}{\\operatorname{argmax}} \\; \\ln \\left( \\prod_{n=1}^{N} \\theta(y_n \\; {\\bf w}^T {\\bf x}_n) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can maximize the above proportional to a constant as well so we'll tack on a $\\frac{1}{N}$:\n",
    "\n",
    "$$\n",
    "\\underset{{\\bf w}}{\\operatorname{argmax}} \\; \\frac{1}{N} \\ln \\left( \\prod_{n=1}^{N} \\theta(y_n \\; {\\bf w}^T {\\bf x}_n) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now maximizing that is the same as minimizing its negative:\n",
    "\n",
    "$$\n",
    "\\underset{{\\bf w}}{\\operatorname{argmin}} \\left[ -\\frac{1}{N} \\ln \\left( \\prod_{n=1}^{N} \\theta(y_n \\; {\\bf w}^T {\\bf x}_n) \\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we move the negative into the log and the log into the product we turn the product into a sum of logs:\n",
    "\n",
    "$$\n",
    "\\underset{{\\bf w}}{\\operatorname{argmin}} \\;\\frac{1}{N} \\sum_{n=1}^{N} \\ln \\left( \\frac{1}{\\theta(y_n \\; {\\bf w}^T {\\bf x}_n)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the logistic function,\n",
    "\n",
    "$$\n",
    "\\underset{{\\bf w}}{\\operatorname{argmin}} \\;\\frac{1}{N} \\sum_{n=1}^{N} \\ln \\left( 1 + e^{y_n \\; {\\bf w}^T {\\bf x}_n} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a much nicer form for the error measure known as the \"cross-entropy\" error.\n",
    "\n",
    "$$\n",
    "E_\\text{in}({\\bf w}) = \\frac{1}{N} \\sum_{n=1}^{N} \\ln \\left( 1+e^{-y_n \\; {\\bf w}^T {\\bf x}_n} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice because it can be interpreted as the average point error where the point error function is\n",
    "\n",
    "$$\n",
    "e(h({\\bf x}_n), y_n) = \\ln \\left( 1+e^{-y_n \\; {\\bf w}^T {\\bf x}_n} \\right) \\\\\n",
    "E_\\text{in}({\\bf w}) = \\frac{1}{N} \\sum_{n=1}^{N} e(h({\\bf x}_n), y_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to learn a hypothesis we'll want to perform the following optimization:\n",
    "\n",
    "$$\n",
    "{\\bf w}_h =\n",
    "\\underset{{\\bf w}}{\\operatorname{argmin}} \\; E_\\text{in}({\\bf w}) =\n",
    "\\underset{{\\bf w}}{\\operatorname{argmin}} \\;\\frac{1}{N} \\sum_{n=1}^{N} \\ln \\left( 1 + e^{y_n \\; {\\bf w}^T {\\bf x}_n} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm is how we search the set of possible hypotheses (hypothesis space $\\mathcal{H}$) for the best parameterization (in this case the weight vector ${\\bf w}$). This search is an optimization problem looking for the hypothesis that optimizes an error measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no sophisticted, closed-form solution like least-squares linear, so we will use gradient descent instead. Specifically we will use batch gradient descent which calculates the gradient from all data points in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, our \"cross-entropy\" error measure is convex so there is only one minimum. Thus the minimum we arrive at is the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a general method and requires twice differentiability for smoothness. It updates the parameters using a first-order approximation of the error surface.\n",
    "\n",
    "$$\n",
    "{\\bf w}_{i+1} = {\\bf w}_i + \\nabla E_\\text{in}({\\bf w}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn we're going to minimize the following error measure using batch gradient descent.\n",
    "\n",
    "$$\n",
    "e(h({\\bf x}_n), y_n) = \\ln \\left( 1+e^{-y_n \\; {\\bf w}^T {\\bf x}_n} \\right) \\\\\n",
    "E_\\text{in}({\\bf w}) = \\frac{1}{N} \\sum_{n=1}^{N} e(h({\\bf x}_n), y_n) = \\frac{1}{N} \\sum_{n=1}^{N} \\ln \\left( 1+e^{-y_n \\; {\\bf w}^T {\\bf x}_n} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the derivative of the point loss function and possibly some abuse of notation.\n",
    "\n",
    "$$\n",
    "\\frac{d}{d{\\bf w}} e(h({\\bf x}_n), y_n)\n",
    "= \\frac{-y_n \\; {\\bf x}_n \\; e^{-y_n {\\bf w}^T {\\bf x}_n}}{1 + e^{-y_n {\\bf w}^T {\\bf x}_n}}\n",
    "= -\\frac{y_n \\; {\\bf x}_n}{1 + e^{y_n {\\bf w}^T {\\bf x}_n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the point loss derivative we can determine the gradient of the in-sample error:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla E_\\text{in}({\\bf w})\n",
    "&= \\frac{d}{d{\\bf w}} \\left[ \\frac{1}{N} \\sum_{n=1}^N e(h({\\bf x}_n), y_n) \\right] \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N \\frac{d}{d{\\bf w}} e(h({\\bf x}_n), y_n) \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N \\left( - \\frac{y_n \\; {\\bf x}_n}{1 + e^{y_n {\\bf w}^T {\\bf x}_n}} \\right) \\\\\n",
    "&= - \\frac{1}{N} \\sum_{n=1}^N \\frac{y_n \\; {\\bf x}_n}{1 + e^{y_n {\\bf w}^T {\\bf x}_n}} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weight update rule per batch gradient descent becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf w}_{i+1} &= {\\bf w}_i - \\eta \\; \\nabla E_\\text{in}({\\bf w}_i) \\\\\n",
    "&= {\\bf w}_i - \\eta \\; \\left( - \\frac{1}{N} \\sum_{n=1}^N \\frac{y_n \\; {\\bf x}_n}{1 + e^{y_n {\\bf w}_i^T {\\bf x}_n}} \\right) \\\\\n",
    "&= {\\bf w}_i + \\eta \\; \\left( \\frac{1}{N} \\sum_{n=1}^N \\frac{y_n \\; {\\bf x}_n}{1 + e^{y_n {\\bf w}_i^T {\\bf x}_n}} \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is our learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enough with the theory, now jump to the implimentation. We will look at 2 libraries for the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the same dataset as UCLA's Logit Regression tutorial to explore logistic regression in Python. Our goal will be to identify the various factors that may influence admission into graduate school.\n",
    "\n",
    "The dataset contains several columns which we can use as predictor variables:\n",
    "\n",
    "   * gpa\n",
    "   * gre score\n",
    "   * rank or prestige of an applicant's undergraduate alma mater\n",
    "   * The fourth column, admit, is our binary target variable. It indicates whether or not a candidate was admitted our not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data in\n",
    "df = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank\n",
       "0      0  380  3.61     3\n",
       "1      1  660  3.67     3\n",
       "2      1  800  4.00     1\n",
       "3      1  640  3.19     4\n",
       "4      0  520  2.93     4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the 'rank' column because there is also a DataFrame method called 'rank'\n",
    "df.columns = [\"admit\", \"gre\", \"gpa\", \"prestige\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics & Looking at the data\n",
    "Now that we've got everything loaded into Python and named appropriately let's take a look at the data. We can use the pandas function which describes a summarized view of everything. There's also function for calculating the standard deviation, std.\n",
    "\n",
    "A feature I really like in pandas is the pivot_table/crosstab aggregations. crosstab makes it really easy to do multidimensional frequency tables. You might want to play around with this to look at different cuts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317500</td>\n",
       "      <td>587.700000</td>\n",
       "      <td>3.389900</td>\n",
       "      <td>2.48500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466087</td>\n",
       "      <td>115.516536</td>\n",
       "      <td>0.380567</td>\n",
       "      <td>0.94446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.395000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre         gpa   prestige\n",
       "count  400.000000  400.000000  400.000000  400.00000\n",
       "mean     0.317500  587.700000    3.389900    2.48500\n",
       "std      0.466087  115.516536    0.380567    0.94446\n",
       "min      0.000000  220.000000    2.260000    1.00000\n",
       "25%      0.000000  520.000000    3.130000    2.00000\n",
       "50%      0.000000  580.000000    3.395000    2.00000\n",
       "75%      1.000000  660.000000    3.670000    3.00000\n",
       "max      1.000000  800.000000    4.000000    4.00000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prestige</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>97</td>\n",
       "      <td>93</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prestige   1   2   3   4\n",
       "admit                   \n",
       "0         28  97  93  55\n",
       "1         33  54  28  12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency table cutting presitge and whether or not someone was admitted\n",
    "pd.crosstab(df['admit'], df['prestige'], rownames=['admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0da9bfd898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot all of the columns\n",
    "df.hist()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### dummy variables\n",
    "pandas gives you a great deal of control over how categorical variables can be represented. We're going dummify the \"prestige\" column using get_dummies.\n",
    "\n",
    "get_dummies creates a new DataFrame with binary indicator variables for each category/option in the column specified. In this case, prestige has four levels: 1, 2, 3 and 4 (1 being most prestigious). When we call get_dummies, we get a dataframe with four columns, each of which describes one of those levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prestige_1</th>\n",
       "      <th>prestige_2</th>\n",
       "      <th>prestige_3</th>\n",
       "      <th>prestige_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prestige_1  prestige_2  prestige_3  prestige_4\n",
       "0           0           0           1           0\n",
       "1           0           0           1           0\n",
       "2           1           0           0           0\n",
       "3           0           0           0           1\n",
       "4           0           0           0           1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummify rank\n",
    "dummy_ranks = pd.get_dummies(df['prestige'], prefix='prestige')\n",
    "dummy_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige_2</th>\n",
       "      <th>prestige_3</th>\n",
       "      <th>prestige_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige_2  prestige_3  prestige_4\n",
       "0      0  380  3.61           0           1           0\n",
       "1      1  660  3.67           0           1           0\n",
       "2      1  800  4.00           0           0           0\n",
       "3      1  640  3.19           0           0           1\n",
       "4      0  520  2.93           0           0           1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a clean data frame for the regression\n",
    "cols_to_keep = ['admit', 'gre', 'gpa']\n",
    "data = df[cols_to_keep].join(dummy_ranks.ix[:, 'prestige_2':])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually add the intercept\n",
    "data['intercept'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, we merge the new dummy columns with the original dataset and get rid of the prestige column which we no longer need.\n",
    "\n",
    "Lastly we're going to add a constant term for our logistic regression. The statsmodels function we would use requires intercepts/constants to be specified explicitly.\n",
    "\n",
    "### Performing the regression\n",
    "Actually doing the logistic regression is quite simple. Specify the column containing the variable you're trying to predict followed by the columns that the model should use to make the prediction.\n",
    "\n",
    "In our case we'll be predicting the admit column using gre, gpa, and the prestige dummy variables prestige_2, prestige_3 and prestige_4. We're going to treat prestige_1 as our baseline and exclude it from our fit. This is done to prevent multicollinearity, or the dummy variable trap caused by including a dummy variable for every single category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573147\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "train_cols = data.columns[1:]\n",
    "# Index([gre, gpa, prestige_2, prestige_3, prestige_4], dtype=object)\n",
    "\n",
    "logit = sm.Logit(data['admit'], data[train_cols])\n",
    "\n",
    "# fit the model\n",
    "result = logit.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing a logistic regression, we're going to use the statsmodels Logit function. For details on other models available in statsmodels, check out their docs here.\n",
    "\n",
    "### Interpreting the results\n",
    "One of my favorite parts about statsmodels is the summary output it gives. If you're coming from R, I think you'll like the output and find it very familiar too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>admit</td>      <th>  No. Observations:  </th>  <td>   400</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   394</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     5</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 09 Sep 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.08292</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>20:24:18</td>     <th>  Log-Likelihood:    </th> <td> -229.26</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -249.99</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>7.578e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gre</th>        <td>    0.0023</td> <td>    0.001</td> <td>    2.070</td> <td> 0.038</td> <td>    0.000</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gpa</th>        <td>    0.8040</td> <td>    0.332</td> <td>    2.423</td> <td> 0.015</td> <td>    0.154</td> <td>    1.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prestige_2</th> <td>   -0.6754</td> <td>    0.316</td> <td>   -2.134</td> <td> 0.033</td> <td>   -1.296</td> <td>   -0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prestige_3</th> <td>   -1.3402</td> <td>    0.345</td> <td>   -3.881</td> <td> 0.000</td> <td>   -2.017</td> <td>   -0.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prestige_4</th> <td>   -1.5515</td> <td>    0.418</td> <td>   -3.713</td> <td> 0.000</td> <td>   -2.370</td> <td>   -0.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -3.9900</td> <td>    1.140</td> <td>   -3.500</td> <td> 0.000</td> <td>   -6.224</td> <td>   -1.756</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  admit   No. Observations:                  400\n",
       "Model:                          Logit   Df Residuals:                      394\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Sat, 09 Sep 2017   Pseudo R-squ.:                 0.08292\n",
       "Time:                        20:24:18   Log-Likelihood:                -229.26\n",
       "converged:                       True   LL-Null:                       -249.99\n",
       "                                        LLR p-value:                 7.578e-08\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "gre            0.0023      0.001      2.070      0.038       0.000       0.004\n",
       "gpa            0.8040      0.332      2.423      0.015       0.154       1.454\n",
       "prestige_2    -0.6754      0.316     -2.134      0.033      -1.296      -0.055\n",
       "prestige_3    -1.3402      0.345     -3.881      0.000      -2.017      -0.663\n",
       "prestige_4    -1.5515      0.418     -3.713      0.000      -2.370      -0.733\n",
       "intercept     -3.9900      1.140     -3.500      0.000      -6.224      -1.756\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset I chose is the [affairs dataset](http://statsmodels.sourceforge.net/stable/datasets/generated/fair.html) that comes with [Statsmodels](http://statsmodels.sourceforge.net/). It was derived from a survey of women in 1974 by Redbook magazine, in which married women were asked about their participation in extramarital affairs. More information about the study is available in a [1978 paper](http://fairmodel.econ.yale.edu/rayfair/pdf/1978a200.pdf) from the Journal of Political Economy.\n",
    "\n",
    "## Description of Variables\n",
    "\n",
    "The dataset contains 6366 observations of 9 variables:\n",
    "\n",
    "* `rate_marriage`: woman's rating of her marriage (1 = very poor, 5 = very good)\n",
    "* `age`: woman's age\n",
    "* `yrs_married`: number of years married\n",
    "* `children`: number of children\n",
    "* `religious`: woman's rating of how religious she is (1 = not religious, 4 = strongly religious)\n",
    "* `educ`: level of education (9 = grade school, 12 = high school, 14 = some college, 16 = college graduate, 17 = some graduate school, 20 = advanced degree)\n",
    "* `occupation`: woman's occupation (1 = student, 2 = farming/semi-skilled/unskilled, 3 = \"white collar\", 4 = teacher/nurse/writer/technician/skilled, 5 = managerial/business, 6 = professional with advanced degree)\n",
    "* `occupation_husb`: husband's occupation (same coding as above)\n",
    "* `affairs`: time spent in extra-marital affairs\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "I decided to treat this as a classification problem by creating a new binary variable `affair` (did the woman have at least one affair?) and trying to predict the classification for each woman.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Sara\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "First, let's load the dataset and add a binary `affair` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dta = sm.datasets.fair.load_pandas().data\n",
    "\n",
    "# add \"affair\" column: 1 represents having affairs, 0 represents not\n",
    "dta['affair'] = (dta.affairs > 0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate_marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>yrs_married</th>\n",
       "      <th>children</th>\n",
       "      <th>religious</th>\n",
       "      <th>educ</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation_husb</th>\n",
       "      <th>affairs</th>\n",
       "      <th>affair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.826086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.799999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.266665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.041666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.266665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.361111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.826086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.826086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.041666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.839996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.545454</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.799999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.187878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.199999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.177776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.446154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.446154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.187878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.187878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.169231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.839996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.169231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.947825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.361111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.041666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.507691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.266665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.799988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.177776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.507691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rate_marriage   age  yrs_married  children  religious  educ  occupation  \\\n",
       "0              3.0  32.0          9.0       3.0        3.0  17.0         2.0   \n",
       "1              3.0  27.0         13.0       3.0        1.0  14.0         3.0   \n",
       "2              4.0  22.0          2.5       0.0        1.0  16.0         3.0   \n",
       "3              4.0  37.0         16.5       4.0        3.0  16.0         5.0   \n",
       "4              5.0  27.0          9.0       1.0        1.0  14.0         3.0   \n",
       "5              4.0  27.0          9.0       0.0        2.0  14.0         3.0   \n",
       "6              5.0  37.0         23.0       5.5        2.0  12.0         5.0   \n",
       "7              5.0  37.0         23.0       5.5        2.0  12.0         2.0   \n",
       "8              3.0  22.0          2.5       0.0        2.0  12.0         3.0   \n",
       "9              3.0  27.0          6.0       0.0        1.0  16.0         3.0   \n",
       "10             2.0  27.0          6.0       2.0        1.0  16.0         3.0   \n",
       "11             5.0  27.0          6.0       2.0        3.0  14.0         3.0   \n",
       "12             3.0  37.0         16.5       5.5        1.0  12.0         2.0   \n",
       "13             5.0  27.0          6.0       0.0        2.0  14.0         3.0   \n",
       "14             4.0  22.0          6.0       1.0        1.0  14.0         4.0   \n",
       "15             4.0  37.0          9.0       2.0        2.0  14.0         3.0   \n",
       "16             4.0  27.0          6.0       1.0        1.0  12.0         3.0   \n",
       "17             1.0  37.0         23.0       5.5        4.0  14.0         5.0   \n",
       "18             2.0  42.0         23.0       2.0        2.0  20.0         4.0   \n",
       "19             4.0  37.0          6.0       0.0        2.0  16.0         5.0   \n",
       "20             5.0  22.0          2.5       0.0        2.0  14.0         3.0   \n",
       "21             3.0  37.0         16.5       5.5        2.0   9.0         3.0   \n",
       "22             3.0  42.0         23.0       5.5        3.0  12.0         5.0   \n",
       "23             2.0  27.0          9.0       2.0        4.0  20.0         3.0   \n",
       "24             4.0  27.0          6.0       1.0        2.0  12.0         5.0   \n",
       "25             5.0  27.0          2.5       0.0        3.0  16.0         4.0   \n",
       "26             2.0  27.0          6.0       2.0        2.0  12.0         2.0   \n",
       "27             5.0  37.0         13.0       1.0        3.0  12.0         3.0   \n",
       "28             2.0  32.0         16.5       2.0        2.0  12.0         4.0   \n",
       "29             3.0  27.0          6.0       1.0        1.0  14.0         3.0   \n",
       "..             ...   ...          ...       ...        ...   ...         ...   \n",
       "970            5.0  32.0         13.0       1.0        2.0  14.0         4.0   \n",
       "971            2.0  32.0          2.5       0.0        3.0  20.0         4.0   \n",
       "972            2.0  27.0          9.0       2.0        2.0  16.0         4.0   \n",
       "973            5.0  22.0          2.5       0.0        2.0  14.0         3.0   \n",
       "974            4.0  27.0          2.5       0.0        2.0  17.0         4.0   \n",
       "975            4.0  27.0          0.5       0.0        3.0  12.0         3.0   \n",
       "976            4.0  32.0         13.0       2.0        4.0  14.0         4.0   \n",
       "977            3.0  27.0          9.0       1.0        3.0  12.0         2.0   \n",
       "978            3.0  32.0         13.0       2.0        2.0  12.0         3.0   \n",
       "979            5.0  27.0          9.0       2.0        2.0  12.0         3.0   \n",
       "980            5.0  22.0          6.0       1.0        2.0  12.0         3.0   \n",
       "981            3.0  32.0         13.0       0.0        1.0  12.0         3.0   \n",
       "982            3.0  37.0         23.0       2.0        3.0  14.0         4.0   \n",
       "983            5.0  32.0         16.5       1.0        3.0  12.0         3.0   \n",
       "984            3.0  32.0         16.5       3.0        1.0  14.0         3.0   \n",
       "985            5.0  22.0          6.0       0.0        1.0  14.0         4.0   \n",
       "986            4.0  37.0         13.0       0.0        2.0  20.0         2.0   \n",
       "987            3.0  42.0         23.0       3.0        2.0  14.0         5.0   \n",
       "988            4.0  22.0          2.5       0.0        3.0  14.0         3.0   \n",
       "989            4.0  27.0         13.0       3.0        3.0  14.0         6.0   \n",
       "990            2.0  42.0         23.0       1.0        1.0  20.0         6.0   \n",
       "991            2.0  27.0          9.0       2.0        3.0  14.0         2.0   \n",
       "992            5.0  27.0          6.0       1.0        2.0  14.0         4.0   \n",
       "993            5.0  37.0         13.0       4.0        2.0  12.0         4.0   \n",
       "994            2.0  22.0          6.0       1.0        2.0  14.0         3.0   \n",
       "995            3.0  17.5          2.5       0.0        4.0  14.0         3.0   \n",
       "996            5.0  22.0          2.5       1.0        3.0  14.0         2.0   \n",
       "997            3.0  27.0          6.0       0.0        3.0  14.0         3.0   \n",
       "998            5.0  27.0          9.0       0.0        1.0  14.0         3.0   \n",
       "999            3.0  32.0         13.0       3.0        2.0  14.0         3.0   \n",
       "\n",
       "     occupation_husb    affairs  affair  \n",
       "0                5.0   0.111111       1  \n",
       "1                4.0   3.230769       1  \n",
       "2                5.0   1.400000       1  \n",
       "3                5.0   0.727273       1  \n",
       "4                4.0   4.666666       1  \n",
       "5                4.0   4.666666       1  \n",
       "6                4.0   0.852174       1  \n",
       "7                3.0   1.826086       1  \n",
       "8                3.0   4.799999       1  \n",
       "9                5.0   1.333333       1  \n",
       "10               5.0   3.266665       1  \n",
       "11               5.0   2.041666       1  \n",
       "12               3.0   0.484848       1  \n",
       "13               2.0   2.000000       1  \n",
       "14               4.0   3.266665       1  \n",
       "15               6.0   1.361111       1  \n",
       "16               5.0   2.000000       1  \n",
       "17               2.0   1.826086       1  \n",
       "18               4.0   1.826086       1  \n",
       "19               4.0   2.041666       1  \n",
       "20               4.0   7.839996       1  \n",
       "21               2.0   2.545454       1  \n",
       "22               4.0   0.532609       1  \n",
       "23               4.0   0.622222       1  \n",
       "24               4.0   0.583333       1  \n",
       "25               1.0   4.799999       1  \n",
       "26               5.0   0.166667       1  \n",
       "27               4.0   0.615385       1  \n",
       "28               2.0   1.187878       1  \n",
       "29               6.0  11.199999       1  \n",
       "..               ...        ...     ...  \n",
       "970              3.0   0.942308       1  \n",
       "971              4.0   1.400000       1  \n",
       "972              5.0   0.111111       1  \n",
       "973              3.0   1.400000       1  \n",
       "974              5.0   0.400000       1  \n",
       "975              4.0   2.000000       1  \n",
       "976              5.0   0.269231       1  \n",
       "977              2.0   2.177776       1  \n",
       "978              5.0   3.446154       1  \n",
       "979              5.0   0.388889       1  \n",
       "980              2.0   1.333333       1  \n",
       "981              4.0   3.446154       1  \n",
       "982              5.0   0.347826       1  \n",
       "983              2.0   1.187878       1  \n",
       "984              6.0   1.187878       1  \n",
       "985              4.0   7.000000       1  \n",
       "986              6.0   5.169231       1  \n",
       "987              4.0   0.521739       1  \n",
       "988              4.0   7.839996       1  \n",
       "989              5.0   5.169231       1  \n",
       "990              6.0   1.947825       1  \n",
       "991              2.0   1.361111       1  \n",
       "992              2.0   2.041666       1  \n",
       "993              5.0   1.507691       1  \n",
       "994              2.0   3.266665       1  \n",
       "995              4.0   3.200000       1  \n",
       "996              4.0  16.799988       1  \n",
       "997              4.0   2.000000       1  \n",
       "998              5.0   2.177776       1  \n",
       "999              6.0   1.507691       1  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate_marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>yrs_married</th>\n",
       "      <th>children</th>\n",
       "      <th>religious</th>\n",
       "      <th>educ</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation_husb</th>\n",
       "      <th>affairs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affair</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.329701</td>\n",
       "      <td>28.390679</td>\n",
       "      <td>7.989335</td>\n",
       "      <td>1.238813</td>\n",
       "      <td>2.504521</td>\n",
       "      <td>14.322977</td>\n",
       "      <td>3.405286</td>\n",
       "      <td>3.833758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.647345</td>\n",
       "      <td>30.537019</td>\n",
       "      <td>11.152460</td>\n",
       "      <td>1.728933</td>\n",
       "      <td>2.261568</td>\n",
       "      <td>13.972236</td>\n",
       "      <td>3.463712</td>\n",
       "      <td>3.884559</td>\n",
       "      <td>2.187243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rate_marriage        age  yrs_married  children  religious       educ  \\\n",
       "affair                                                                          \n",
       "0            4.329701  28.390679     7.989335  1.238813   2.504521  14.322977   \n",
       "1            3.647345  30.537019    11.152460  1.728933   2.261568  13.972236   \n",
       "\n",
       "        occupation  occupation_husb   affairs  \n",
       "affair                                         \n",
       "0         3.405286         3.833758  0.000000  \n",
       "1         3.463712         3.884559  2.187243  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta.groupby('affair').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on average, women who have affairs rate their marriages lower, which is to be expected. Let's take another look at the `rate_marriage` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>yrs_married</th>\n",
       "      <th>children</th>\n",
       "      <th>religious</th>\n",
       "      <th>educ</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation_husb</th>\n",
       "      <th>affairs</th>\n",
       "      <th>affair</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_marriage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>33.823232</td>\n",
       "      <td>13.914141</td>\n",
       "      <td>2.308081</td>\n",
       "      <td>2.343434</td>\n",
       "      <td>13.848485</td>\n",
       "      <td>3.232323</td>\n",
       "      <td>3.838384</td>\n",
       "      <td>1.201671</td>\n",
       "      <td>0.747475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>30.471264</td>\n",
       "      <td>10.727011</td>\n",
       "      <td>1.735632</td>\n",
       "      <td>2.330460</td>\n",
       "      <td>13.864943</td>\n",
       "      <td>3.327586</td>\n",
       "      <td>3.764368</td>\n",
       "      <td>1.615745</td>\n",
       "      <td>0.635057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>30.008056</td>\n",
       "      <td>10.239174</td>\n",
       "      <td>1.638469</td>\n",
       "      <td>2.308157</td>\n",
       "      <td>14.001007</td>\n",
       "      <td>3.402820</td>\n",
       "      <td>3.798590</td>\n",
       "      <td>1.371281</td>\n",
       "      <td>0.550856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>28.856601</td>\n",
       "      <td>8.816905</td>\n",
       "      <td>1.369536</td>\n",
       "      <td>2.400981</td>\n",
       "      <td>14.144514</td>\n",
       "      <td>3.420161</td>\n",
       "      <td>3.835861</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.322926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>28.574702</td>\n",
       "      <td>8.311662</td>\n",
       "      <td>1.252794</td>\n",
       "      <td>2.506334</td>\n",
       "      <td>14.399776</td>\n",
       "      <td>3.454918</td>\n",
       "      <td>3.892697</td>\n",
       "      <td>0.348174</td>\n",
       "      <td>0.181446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age  yrs_married  children  religious       educ  \\\n",
       "rate_marriage                                                           \n",
       "1.0            33.823232    13.914141  2.308081   2.343434  13.848485   \n",
       "2.0            30.471264    10.727011  1.735632   2.330460  13.864943   \n",
       "3.0            30.008056    10.239174  1.638469   2.308157  14.001007   \n",
       "4.0            28.856601     8.816905  1.369536   2.400981  14.144514   \n",
       "5.0            28.574702     8.311662  1.252794   2.506334  14.399776   \n",
       "\n",
       "               occupation  occupation_husb   affairs    affair  \n",
       "rate_marriage                                                   \n",
       "1.0              3.232323         3.838384  1.201671  0.747475  \n",
       "2.0              3.327586         3.764368  1.615745  0.635057  \n",
       "3.0              3.402820         3.798590  1.371281  0.550856  \n",
       "4.0              3.420161         3.835861  0.674837  0.322926  \n",
       "5.0              3.454918         3.892697  0.348174  0.181446  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta.groupby('rate_marriage').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An increase in `age`, `yrs_married`, and `children` appears to correlate with a declining marriage rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with histograms of education and marriage rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of education\n",
    "dta.educ.hist()\n",
    "plt.title('Histogram of Education')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of marriage rating\n",
    "dta.rate_marriage.hist()\n",
    "plt.title('Histogram of Marriage Rating')\n",
    "plt.xlabel('Marriage Rating')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of marriage ratings for those having affairs versus those not having affairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# barplot of marriage rating grouped by affair (True or False)\n",
    "pd.crosstab(dta.rate_marriage, dta.affair.astype(bool)).plot(kind='bar')\n",
    "plt.title('Marriage Rating Distribution by Affair Status')\n",
    "plt.xlabel('Marriage Rating')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a stacked barplot to look at the percentage of women having affairs by number of years of marriage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Percentage')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEjCAYAAADdZh27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XecFeW9x/HPlybFggJWqhELJrYgxphEvTa4sV1jwGgEI4hdE703YqJEjSZqvIkmEmvsFbkxomKJJmjUoKyKBbEgoqwVFbABgvzuHzM7HA5bzsLOni3f9+u1r9eUZ57zm7O785t5nplnFBGYmZkBtCl3AGZm1nQ4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFFoQSZ0k3S1pgaQ70mXnSvpQ0nslbH+5pDPzj7R1kLSbpMpyx9FSSBoh6b5V3HaUpMkNHFKL5KTQDEmaLGmepDWKVh0MbAB0i4gfSuoFnAoMiIgN66o3Io6JiF/XI44jJH0l6TNJn0iaJmnfeu1MztIYHyt3HKtD0lGSZhT+viV1k/SBpMFljGszSSHpqaLlG0haImlmQ35eRFwfEUMask5bmZNCMyOpL/BdIID9i1b3AV6NiKUF8x9FxAcN8Lntalj174hYE+gK/AUYL2m9BqrbgIi4CqgExhYsvhiYFBH3N+RnreLvYh1JWxXMHwbMasgY/DfSeJwUmp/hwBTgOmBE1UJJZ5McNIalZ+5HA38HNk7nr0vL3SHpvbSJ6VFJWxfUcZ2kc9Pp3SRVSjotbXq6tragImIZcA3QCdg0rWPf9OphvqQnJG1T8Fmz07qfBz6X1E5SL0l/lTRX0keSLi0of2R6tjxP0gOS+hSsC0nHSHotXT9Oia2Ay4Gd0+9gflr++5KeTa9u5kg6q3BfJA2X9GYaw5lprHum69pIGiPp9XR9nUlQ0i/SJrzZkg5Ll+0o6f3Cg52kH0iaVkM1RwHHSdpO0t7AHsDPCrbdX9Jz6Xf9mKSvF6w7Q9IsSZ9Kmi5p/4J1o9K/gz9K+hg4Q9Lm6bIFady31LZ/wI0kf5dVhgM3FH0H9Y2hpmWTC7YbIOkhSR9LelnSDwrW9ZB0T/o7ngL0q2MfrEpE+KcZ/QAzgeOAbwJLgA0K1p0F3FQwvxtQWbT9kcBawBokZ5vTCtZdB5xbsO1S4IK0bKdqYjkCeCydbgecDHwKrAPsAHwA7AS0JUlgs4E10vKzgWlAL5JE0hZ4DvgD0AXoCHwnLXtgut9bpZ9zBvBEQRwB3ENytdIbmAsMLo6x6Hv5BslJ0TbA+8CB6boBwGfAd4AOwEXp97xnuv6nJEm5Z/q9XAHcWsPvquo7/H1adlfgc2CLdP1LwJCC8ncCp9byuz8ReAZ4oyredPmO6T7smH6PRwKvAx3S9UOBjdL9PTTdvw3SdaPSGI9Nt+0E3AGclpbvCOxSQzybpd99H+DNtPw3gOnAYGBmQdn6xlDTssnpNmsBb5MkoHYk/w8fFXy3E4Bbgc7p7/jdqm39U8cxptwB+Kcev6zkQLUE6J7Ovwz8rGD9WdSRFIrq65r+U6+Tzl/HiknhS6BjLdsfkf7jzgc+JDlYVh08LwN+XVT+FWDXdHo2cGTBup1JDubtqvmc+4CRBfNtgC+APul8kCaQdH48MKYgxsdq2oe0zMXAH9LpsRQc5NODypcF+zUD2KNg/Ubp76S6uHdLv58uRbGdmU6fBtycTq+X7tNGtcQp4EngzqLlVwG/Klr2OjUfzF8Evp9OjwJmFa2/Jf39bVLH97YZEOn0ZJKrl4vS/VohKaxCDDUtm5xOHwb8s2j9X4BfAu3T732zgnUX4qRQ0o+bj5qXEcCDEfFhOn8LBU1IdZHUVtL5adPHJyQHZoDuNWwyNyIW1VHtlIjoGhHdI+JbEfFQurwPcGranDE/bbrpBWxcsO2cgulewJuxvD+kUB/gkoJ6PiY5QG5SUKbw7qovgDVrCljSTpL+mTZTLQCOYfl3sHFhXBHxBckZaGEsdxbEMgP4iqSDvzrzIuLzgvk3Wf4d3ATsJ2lNkjPpf0XEuzXFnR59Z5CciRfqA5xW9F1vRPr9KOlsf65g3Zas+DufU1TfqSQH1gpJL0gq5W/sBuAnwLB0v1awCjHUtKxKH2CXon0eRrLfG5BcXRRu/2YJ+2Akl13WDEjqRHLgaKvlt5euAXSVtG1EPFdCNYcCBwB7kiSEdYB5JAfY6qzOELpzgPMi4rxayhTWPwfoLaldNYmhqq6bVyGO6vbhFuBSkqabRZIuZvkB6l1gi6qC6fferSiWIyPi8RI/f11JXQoSQ2+Ss2Qi4m1J/wb+Czic5Ox8VcwBzo6IC4pXSNo0rXcP4MmI+ErSi6z4O1/hO0oT06h0++8Bf5f0aES8UUsMdwB/JLnx4G1J31idGGpZVmUO8HBUczeSpPbAMpITjao7oHrXUpcV8JVC83EgyRnpAGC79Gcr4F+s2MlXm7WAxSRnvp2B3zR8mJmrgGPSs3JJ6pJ28K5VQ/mnSA7I56dlO0raJV13OXC60k5xSetI+mGJcbwP9JTUoWDZWsDHaUIYRJIsq0wgOXv/drrN2ax48LocOE9pR3faoXlAHTGcLamDpO8C+5IcQKvcAPycpC3+zhL3qdiVwPFp57UkrSlpP0ldSK6YgqRpTpJGkZyl10jSUElVV2Hz0+2/qm2biPgU2B04uprV9Y6hBBOBrSUdKql9+jNI0hYRsQT4G8n33klJp/vhq/l5rYaTQvMxArg2It6KiPeqfkjOeA9Tabfs3UByGf02SSfnlLyCjYgKkjtmLiW5GplJ0r5fU/mvgP1I2qnfIrkFc1i67k6SDu/b0mavF4FS71f/B0lzy3uSqprdjgPOkfQpSR/C+II4ppN06N5GkqQ+JekwX5wWuYTkgPRguv0Uks70mrxHsv/vADcDx0TEywXr7yRtkipqZipZRDxJ0iF7WfpZrwI/Ttc9T3IGX5V0tyTpl6jNTsBUSZ8DfwWOj4i3SohjakSsdCvqKsZQ12ctAPYh2c93Sb7n35JcPUPyfaxLclLwF+q4e86WU9oJY2bVSNv75wP962g+WZ3PeB04uqA/xqxsfKVgViRteumcNr9cBLzA8k75hv6sH5A0rfwjj/rN6ssdzWYrO4DkgSwBFcAhkcMldfog1gDg8Ege/jMrOzcfmZlZxs1HZmaWcVIwM7NMs+tT6N69e/Tt27fcYZiZNStPP/30hxHRo65yzS4p9O3bl4qKinKHYWbWrEgqaagPNx+ZmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllcksKkq6R9EE6bnp165W+f3WmpOcl7ZBXLGZmVpo8rxSuI3klX02GAP3Tn9Gs+gtGzMysgeSWFCLiUZLXJtbkAOCGSEwheYPYRnnFY2ZmdSvnw2ubsOI7VCvTZSu9o1bSaJKrCXr3rudb9c5aZ5UDLK3+BTnX7/hrr78Zx9+cYwfHX2f9zTP+cnY0V/de4GqHbI2IKyNiYEQM7NGjzqe0zcxsFZUzKVSSvFi7Sk+SVxaamVmZlDMpTASGp3chfQtYEBErNR2ZmVnjya1PQdKtwG5Ad0mVwK+A9gARcTkwCfhPkhe6fwH8JK9YzMysNLklhYj4UR3rAzg+r883M7P68xPNZmaWcVIwM7OMk4KZmWWa3ZvX6qvvoltyrX92rrWbmTUuXymYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzTK5JQdJgSa9ImilpTDXre0v6p6RnJT0v6T/zjMfMzGqXW1KQ1BYYBwwBBgA/kjSgqNgZwPiI2B44BPhzXvGYmVnd8rxSGATMjIhZEfElcBtwQFGZANZOp9cB3skxHjMzq0O7HOveBJhTMF8J7FRU5izgQUknAl2APXOMx8zM6pDnlYKqWRZF8z8CrouInsB/AjdKWikmSaMlVUiqmDt3bg6hmpkZ5JsUKoFeBfM9Wbl5aCQwHiAi/g10BLoXVxQRV0bEwIgY2KNHj5zCNTOzPJPCVKC/pH6SOpB0JE8sKvMWsAeApK1IkoIvBczMyiS3PoWIWCrpBOABoC1wTURMl3QOUBERE4FTgask/YykaemIiChuYmrV+i66Jdf6Z+dau5k1N3l2NBMRk4BJRcvGFky/BOySZwxmZlY6P9FsZmYZJwUzM8s4KZiZWcZJwczMMk4KZmaWcVIwM7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZZwUzMws067cAVjL1nfRLbnWPzvX2s1an5KvFCR1krRFnsGYmVl5lZQUJO0HTAPuT+e3kzQxz8DMzKzxlXqlcBYwCJgPEBHTgL75hGRmZuVSalJYGhELco3EzMzKrtSO5hclHQq0ldQfOAl4Ir+wzMysHEq9UjgR2BpYDNwKfAL8NK+gzMysPEq6UoiIL4Bfpj9mZtZClZQUJN0NRNHiBUAFcEVELKphu8HAJUBb4OqIOL+aMkNJOrIDeC4iDi05ejMza1Cl9inMAnqQNB0BDAPeBzYHrgIOL95AUltgHLAXUAlMlTQxIl4qKNMfOB3YJSLmSVp/VXfEzMxWX6lJYfuI+F7B/N2SHo2I70maXsM2g4CZETELQNJtwAHASwVljgLGRcQ8gIj4oH7hm5lZQyq1o7mHpN5VM+l093T2yxq22QSYUzBfmS4rtDmwuaTHJU1Jm5vMzKxMSr1SOBV4TNLrgIB+wHGSugDX17CNqllW3C/RDugP7Ab0BP4l6esRMX+FiqTRwGiA3r17Y2Zm+Sj17qNJafv/liQH+5cLOpcvrmGzSqBXwXxP4J1qykyJiCXAG5JeIUkSU4s+/0rgSoCBAwcWJxYzM2sg9Rk6uz+wBbANMFTS8DrKTwX6S+onqQNwCFA8XtLfgN0BJHUnaU6aVY+YzMysAZV6S+qvSJp4BgCTgCHAY8ANNW0TEUslnQA8QHJL6jURMV3SOUBFRExM1+0t6SXgK+B/IuKj1dgfM0t52HJbFaX2KRwMbAs8GxE/kbQBcHVdG0XEJJIkUrhsbMF0AKekP2ZmVmalJoWFEbFM0lJJawMfAJvmGJdZk5Dn2fbs3Gq2pqC5XqmVmhQqJHUleVDtaeAz4KmcYjIzszIp9e6j49LJyyXdD6wdEc/nF5aZmZVDqW9ee7hqOiJmR8TzhcvMzKxlqPVKQVJHoDPQXdK6LH8gbW1g45xjMzOzRlZX89HRJO9N2JikL6EqKXxCMtidmZm1ILUmhYi4BLhE0okR8adGisnMrNnevdPcldrR/CdJ3wb6Fm4TETU+vGZmZs1PqU803wh8DZhG8uQxJIPbOSmYmbUgpT6nMBAYkD6BbGZmLVSpA+K9CGyYZyBmZlZ+pV4pdAdekvQUsLhqYUTsn0tUZmZWFqUmhbPyDMLMzJqGUu8+ekRSH6B/RDwkqTPJcNhmZtaClDrMxVHABOCKdNEmJC/IMTOzFqTUjubjgV1InmQmIl4D1s8rKDMzK49Sk8LiiPiyakZSO5LnFMzMrAUpNSk8IukXQCdJewF3AHfnF5aZmZVDqUlhDDAXeIFkkLxJwBl5BWVmZuVR6i2pnYBrIuIqAElt02Vf5BWYmZk1vlKvFB4mSQJVOgEPNXw4ZmZWTqUmhY4R8VnVTDrdOZ+QzMysXEpNCp9L2qFqRtI3gYX5hGRmZuVSap/CycAdkt5J5zcChuUTkpmZlUudSUFSG6ADsCWwBckrOV+OiCU5x2ZmZo2szqQQEcsk/W9E7EwyhLaZmbVQpfYpPCjpB5KUazRmZlZWpfYpnAJ0Ab6StJCkCSkiYu3cIjMzs0ZX6tDZa+UdiJmZlV+pQ2dL0o8lnZnO95I0KN/QzMyssZXap/BnYGfg0HT+M2BcLhGZmVnZlNqnsFNE7CDpWYCImCepQ45xmZlZGZR6pbAkHQQvACT1AJblFpWZmZVFqUnhj8CdwPqSzgMeA35T10aSBkt6RdJMSWNqKXewpJA0sMR4zMwsB6XefXSzpKeBPUhuRz0wImbUtk16ZTEO2AuoBKZKmhgRLxWVWws4CXhyFeI3M7MGVGtSkNQROAbYjOQFO1dExNIS6x4EzIyIWWldtwEHAC8Vlfs1cCHw3/WI28zMclBX89H1wECShDAEuKgedW8CzCmYr0yXZSRtD/SKiHvqUa+ZmeWkruajARHxDQBJfwGeqkfd1Q2JEdnKZKC9PwBH1FmRNBoYDdC7d+96hGBmZvVR15VCNhJqPZqNqlQCvQrmewLvFMyvBXwdmCxpNvAtYGJ1nc0RcWVEDIyIgT169KhnGGZmVqq6rhS2lfRJOi2gUzpfythHU4H+kvoBbwOHsPzhNyJiAdC9al7SZOC/I6Ki3nthZmYNotakEBFtV7XiiFgq6QTgAaAtcE1ETJd0DlARERNXtW4zM8tHqU80r5KImARMKlo2toayu+UZi5mZ1a3Uh9fMzKwVcFIwM7OMk4KZmWWcFMzMLOOkYGZmGScFMzPLOCmYmVnGScHMzDJOCmZmlnFSMDOzjJOCmZllnBTMzCzjpGBmZhknBTMzyzgpmJlZxknBzMwyTgpmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMu3KHYBZQ1qyZAmVlZUsWrSoQeq7av+NGqSe6syYMaPB6urYsSM9e/akffv2DVantU5OCtaiVFZWstZaa9G3b18krXZ9SyrnN0BU1duqZ9cGqSci+Oijj6isrKRfv34NUqe1Xm4+shZl0aJFdOvWrUESQnMhiW7dujXY1ZG1bk4K1uK0poRQpTXus+XDScHMzDJOCmb19OA9f+PA3Xdi5ND9ADjt+JEcvNcu3HjVn2vcZvyN13D3hNsaK0SzVeaOZrN6uvO2m/jFeRcx6Nvf5cMP3ue5p5/i/ikv1LrN0MOPrHb50qVLadfO/4bWdPiv0awWPx15GO+9+zaLFy/msCOP5sO5H/Ds1Cm8ffqb7LbXYJ545B98/OGHDN3nu4w55wLeeP01/u/m61my5Et69d2U8y65nE6dOnPZ78+nc+cujDjmREb+cF+2/eYgXnvhafbff39OPfXUcu+mWSbXpCBpMHAJ0Ba4OiLOL1p/CjAKWArMBY6MiDfzjMmsPs6+6FLWWXddFi1cyKH7/gfXTLiXqY8/yiln/Jqtt92eYSOO4sQjhjH+gX8BsOnmW/KDQ0cAcOmF53LnbTdx6E9Gr1Tvp58s4JFHHmnUfTErRW5JQVJbYBywF1AJTJU0MSJeKij2LDAwIr6QdCxwITAsr5jM6uuWa6/gH/ffA8D7777NW2+8Xmv5mS/P4NLfncunnyzgiy8+59u7/ke15fbZ76AGj9WsIeR5pTAImBkRswAk3QYcAGRJISL+WVB+CvDjHOMxq5fJkycz5bHJ3HDXg3Tq1JmRP9yXxYsX17rNmacex8VX38QWA77BXeNvoeLfj1VbrlPnznmEbLba8rz7aBNgTsF8ZbqsJiOB+3KMx6xeFixYwNrrdKVTp868MfNVnn+2os5tvvjsM7qvvyFLlixh0t/uaIQozRpWnlcK1T1NE9UWlH4MDAR2rWH9aGA0QO/evRsqPrNaDR48mN9d/CcO3msX+n6tP9tsP7DObY7/71/w4/33ZONNerHZlgP44rPPGiFSs4aTZ1KoBHoVzPcE3ikuJGlP4JfArhFR7bV5RFwJXAkwcODAahOLWUNbY401+PONE1ZavuMd92TTm/TqzV8f/nc2P3T4SIYOH7nSNseeMiab/kvB9mZNTZ7NR1OB/pL6SeoAHAJMLCwgaXvgCmD/iPggx1jMzKwEuSWFiFgKnAA8AMwAxkfEdEnnSNo/LfY7YE3gDknTJE2soTozM2sEuT6nEBGTgElFy8YWTO+Z5+ebmVn9eOwjMzPLOCmYmVnGScHMzDIeEM9atL5j7m3Q+iaesEudZbbv043+Ww7I5v9w9c1s0qv652tmz57Nvvvuy4svvthgMZqtDicFswa2RsdO2QB5Zs2Nm4/MGsHbc97iiIOGMGzIrgwbsivTKp5cqcz06dMZNGgQ2223Hdtssw2vvfYaADfddFO2/Oijj+arr75q7PCtFXFSMGtgixctZOg+32XoPt/lp6OSMR7X696dK265k9vve4QL/3wNF4wds9J2l19+OSeffDLTpk2joqKCnj17MmPGDG6//XYef/xxpk2bRtu2bbn55psbe5esFXHzkVkDq675aOmSJfz2zJ/zyvQXaNu2LW/OWnkI7p133pnzzjuPyspKDjroIPr378/DDz/M008/zY477gjAwoULWX/99RtlP6x1clIwawQ3XX0Z3bqvzx0PPsayZcsYtNmGK5U59NBD2Wmnnbj33nvZZ599uPrqq4kIRowYwW9/+9syRG2tkZuPzBrBZ598Qvf1N6BNmzbc83+3V9svMGvWLDbddFNOOukk9t9/f55//nn22GMPJkyYwAcfJEODffzxx7z5pl9OaPnxlYK1aLPP//5qbf985fwGiWPoiJGcOno4f7/3Lnb89nfo1LnLSmVuv/12brrpJtq3b8+GG27I2LFjWW+99Tj33HPZe++9WbZsGe3bt2fcuHH06dOnQeIyK+akYNbAprxSudKyPv2+xoS/P57NnzzmVwD07ds3e0bh9NNP5/TTT19p22HDhjFsmN9Sa43DzUdmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8v4llRr2c5aZ7U236Zo/vlRtT84Nn/ex4w+5AAAPpz7AW3atGW9bt0AuPnuh2nfocNqxWOWNycFswbUdd31snGPLvv9+XTu3IURx5y4QpmIICLKEZ5Zndx8ZNYI3npjFgftsTO/Pv1nDBuyK++9U0nXrl2z9bfddhujRo0C4P333+eggw5i4MCBDBo0iClTppQrbGuFnBTMGsms117hv4Ydzvj7H2X9DTeusdxJJ53Ez3/+cyoqKhg/fnyWLMwag5uPzBpJrz79+Pp2O9RZ7qGHHuKVV17J5ufNm8fChQvp1KlTnuGZAU4KZo2mU+fO2XSbNm1W6FdYtGhRNh0RPPXUU3Rwp7SVgZuPzMqgTZs2rLvuurz22mssW7aMO++8M1u35557Mm7cuGx+2rRp5QjRWilfKVjLdtaC1dq8oYbOrs4FF1zA4MGD6d27NwMGDGDx4sUAjBs3jmOPPZZrr72WpUuXsvvuu6+QJMzy5KRglpNjT1n+Hube/TZd6RWdNQ2J3aNHDyZMmJB7fGbVcfORmZllnBTMzCzjpGAtTmt8Wrg17rPlw0nBWpSOHTvy0UcftaqDZETw0Ucf0bFjx3KHYi2AO5qtRenZsyeVlZXMnTu3Qep7f97CBqmnOjM+bbiH0Tp27EjPnj0brD5rvZwUrEVp3749/fr1a7D6hoy5t8HqKjb7/O/nVrfZqsq1+UjSYEmvSJopaUw169eQdHu6/klJffOMx8zMapdbUpDUFhgHDAEGAD+SNKCo2EhgXkRsBvwBuCCveMzMrG55XikMAmZGxKyI+BK4DTigqMwBwPXp9ARgD0nKMSYzM6uF8rpLQ9LBwOCIGJXOHw7sFBEnFJR5MS1Tmc6/npb5sKiu0cDodHYL4BXy0x34sM5STZfjL5/mHDs4/nLLO/4+EdGjrkJ5djRXd8ZfnIFKKUNEXAlc2RBB1UVSRUQMbIzPyoPjL5/mHDs4/nJrKvHn2XxUCfQqmO8JvFNTGUntgHWAj3OMyczMapFnUpgK9JfUT1IH4BBgYlGZicCIdPpg4B/Rmp46MjNrYnJrPoqIpZJOAB4A2gLXRMR0SecAFRExEfgLcKOkmSRXCIfkFU89NEozVY4cf/k059jB8Zdbk4g/t45mMzNrfjz2kZmZZZwUzMws46RgZmYZJwUzM8t4lNQWRtJ6EdFsnvWQtAGwCclDi+9ExPtlDqlemnv8Vj6S1gEGU/D3AzwQEfPLGVervlKQdGTBdE9JD0uaL+kJSZuXM7ZSSDqjYHqApFeBpyXNlrRTGUOrk6TtJE0BJgMXAr8DHpE0RdIOZQ2uBM09fkgOSpKGSTpF0s/S6a7ljmtVSPpOuh97lzuWUkgaDjwD7AZ0BroAu5P8/w4vY2jJW5ta6w/wTMH0eOBokkT5X8DD5Y6vnvHfCwxJpwcBT5Q7vjpin0YyzlXx8m8Bz5U7vlYQ/3DgdeAy4Iz05/J02fByx1dC/E8VTB+V/j5+BTwOjCl3fCXE/wrQtZrl6wKvljM2Nx8tt3lEDE2n75Q0tqzR1N/GEXEfQEQ8JanhXuuVjy4R8WTxwoiYIqlLOQKqp+Ye/y+Bb0ZRU4WkdYEngRvKElXp2hdMjwb2ioi5ki4CpgDnlyeskolqxnkDllH9mHCNprUnhZ6S/kjyS+ghqX1ELEnXta9lu6ZiU0kTSeLvKalzRHyRrmvq8d8n6V6Sg8+cdFkvkjPY+8sWVemae/xN9qBUojZpAmtD8hDuXICI+FzS0vKGVpLzgGckPcjyv5/ewF7Ar8sWFU4K/1MwXQGsCcyTtCErj9PUFBW/n6INZJ2flzV+OKWLiJMkDSHZh01IDkSVwLiImFTW4ErQ3OOnCR+USrQO8DRpcpO0YUS8J2lNmkFSi4jr0xO6fVj+9zMZOD0i5pUzNg9zYdZKpWfahQelSpK7X8p6UFodkjoDG0TEG+WOpblyUqiBpH0j4p5yx7GqJI2O5D0UzU5zjh2af/xWXpKujIjRdZfMR6u+JbUOO5Y7gNXU5C+ha9GcY4dmHr+kZp3QJDXbk7nUFeX88FZ/pSBpS5a3C1c9QDIxImaUNbASpfFvAjwZEZ8VLB8cEU22wzN9jmJGRHyS3ik1BtgBeAn4TUQsKGuAJZD0NZLbl3sBS4HXgFubQ+y1kfTNiHi63HGsKkkbRcS75Y6ryKbAAAAGfUlEQVSjuWrVVwqSTgNuIzmze4rkxUACbpU0ppyxlULSScBdwInAi5IKO55/U56oSnYNUHWn1CUkHYcXpMuuLVdQpUq/+8uBjiRXlZ1IksO/Je1WxtBWW3NNCJK6ATSHhCBpQ0mXSRonqZuksyS9IGm8pI3KGltrvlJInwDeuuA21KrlHYDpEdG/PJGVRtILwM4R8ZmkvsAE4MaIuETSsxGxfVkDrIWkGRGxVTr9TETsULBuWkRsV77o6pZ+99tFxFdp5+akiNhNUm/grqb83UNyUCJ52GsZMJbkxOIHwAzg5KZ+YJV0PnBRRHwoaSDJw6fLSG7FHh4Rj5Q1wDpIup/kgdMuwKHAzcCtJK0We0ZE8Z2FjaZVXymQ/BFtXM3yjdJ1TV3bqiajiJhN8sj8EEm/p+m3a78o6Sfp9HPpPzbp8CJLat6sSam6pXsNYC2AiHiLpv+MCMB1JE11c4B/AguB7wP/IrkCauq+HxEfptO/A4ZFxGYkt9T+b/nCKtkGEfGniDif5MnmCyLirYj4E9CnnIG19ucUfgo8LOk1VrxXezPghLJFVbr3JG0XEdMA0iuGfUmaZr5R3tDqNAq4JB2/6UOSZpc5JL+HUWWNrDRXA1PT8Y++R9L0haQeJK+Wbeo2SA9ASDouIi5Il/9J0sgyxlWq9pLaRcRSoFNETAWIiFclrVHm2EpReEJe/PR428YMpFirbj4CkNSGZKygwnu1p0bEV2UNrASSegJLI+K9atbtEhGPlyGsepG0FrApyQlKZTSjUUYlbQ1sBbwYES+XO576kPRcRGybTp8bEYWDK74QEU36pELSicB+JMNZfA/oCvwV2APYNCIOL2N4dVLyrvoLC28OSZdvBpwfEQeXJzInBbNWqSkflEqVdugfC2xOclIxB/gbcE16BdGkNdU7B50UzGwFkn4SEU3+DrCaNIf40yudE0g69rcj6dy/K123wo0XjR6bk4KZFZL0VkT0Lnccq6o5xN+U7xxs7R3NZq2SpOdrWgVs0JixrIrmHj9Fdw6mTWETJPXBQ2ebWRlsQDIYXvHgdwKeaPxw6q25x99k7xx0UjBrne4B1qw6KBWSNLnxw6m35h7/cJKhUTJp5/hwSR77yMzMmobW/kSzmZkVcFIwM7OMk4I1e0o8lr4es2rZ0HTQsbw/e09JIWlEwbId02U/Xc26d5L0h3pu85ikJj2YoDVtTgrW7EXSMXYM8HtJHSV1IXkH8fGrU6+kUm/EeAE4pGD+EOC51fmsdFyfJyPiZ/Wpx2x1+e4jaxEi4kVJdwOnkQxHfENEvJ6ewR8PdCC5VfGEiFiWvl1sB5L3INweEecASKokefPVYODidHypo0hGbn0hIn5czcfPAnpI6g58RDJS531VKyUdA4xMY3iVZGjnhZJuAt5P45gq6UugB8lYUO9Juj6N90AlL6S/FBhAMgrr2Ii4Ox22+3pgC5JRTzuu/rdprZmTgrUkZwPPAF8CAyV9neTNaN+OiKVpIjgEuAUYExEfp2fo/5Q0ISJeSuv5PCJ2AZD0LtAnIr6U1LWWz/4/4GCSYQueZMXhv++IiMvT+s4HjgAuS9d9DdgjTVTnAtsD34uIRZL2LKhjLHB/RBwhaV3gSUl/JxkqYV5EbCNpe6Civl+aWSEnBWsxIuJzSbcDn0XE4vSguiNQIQmSq4KqIdJ/lA4R3Y7knRoDSM60AW4vqHY6cJOku0gGW6vJ7cCNJFcCtwL/UbBum3QAuq4k710ofIfwHRFR+O6OuyJiUTX1703yroyqNwJ2JBnm/XvAhen+Pytpei0xmtXJScFammUsf0GSSEbMPLOwgKT+wMnAoIiYnzbjFDa7fF4wvQ+wK8kbsc6Q9PXqhlWPiLeVZJ5dgeNYMSncAAxJm7hGAd+q4bOqm8/CBg6MiNeL9gWSd4ubNQh3NFtL9hAwNG3rR8m7cHsDawOfAp8oeR/uPtVtLKkt0DMi/gH8D0l7f+daPu9M4LSiM39I+jjek9Se5NWLq+IB4KSC2KoGTHsUOCxdti2w9SrWbwb4SsFasIh4QdLZwEPpy5SWkNylVEHSVPQiSSdxTS8jagfckr4IqA1wQUR8WsvnPVbDqrHAU8Bb6WeuSmfw2SQd3y+kscwkuXq5FLg+HSDuGdynYKvJw1yYmVnGzUdmZpZxUjAzs4yTgpmZZZwUzMws46RgZmYZJwUzM8s4KZiZWcZJwczMMv8PaDLKYCUJupkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "affair_yrs_married = pd.crosstab(dta.yrs_married, dta.affair.astype(bool))\n",
    "affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Affair Percentage by Years Married')\n",
    "plt.xlabel('Years Married')\n",
    "plt.ylabel('Percentage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Logistic Regression\n",
    "\n",
    "To prepare the data, I want to add an intercept column as well as dummy variables for `occupation` and `occupation_husb`, since I'm treating them as categorial variables. The dmatrices function from the [patsy module](http://patsy.readthedocs.org/en/latest/) can do that using formula language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Intercept', 'C(occupation)[T.2.0]', 'C(occupation)[T.3.0]',\n",
       "       'C(occupation)[T.4.0]', 'C(occupation)[T.5.0]', 'C(occupation)[T.6.0]',\n",
       "       'C(occupation_husb)[T.2.0]', 'C(occupation_husb)[T.3.0]',\n",
       "       'C(occupation_husb)[T.4.0]', 'C(occupation_husb)[T.5.0]',\n",
       "       'C(occupation_husb)[T.6.0]', 'rate_marriage', 'age', 'yrs_married',\n",
       "       'children', 'religious', 'educ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframes with an intercept column and dummy variables for\n",
    "# occupation and occupation_husb\n",
    "y, X = dmatrices('affair ~ rate_marriage + age + yrs_married + children + \\\n",
    "                  religious + educ + C(occupation) + C(occupation_husb)',\n",
    "                  dta, return_type=\"dataframe\")\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names for the dummy variables are ugly, so let's rename those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix column names of X\n",
    "X = X.rename(columns = {'C(occupation)[T.2.0]':'occ_2',\n",
    "                        'C(occupation)[T.3.0]':'occ_3',\n",
    "                        'C(occupation)[T.4.0]':'occ_4',\n",
    "                        'C(occupation)[T.5.0]':'occ_5',\n",
    "                        'C(occupation)[T.6.0]':'occ_6',\n",
    "                        'C(occupation_husb)[T.2.0]':'occ_husb_2',\n",
    "                        'C(occupation_husb)[T.3.0]':'occ_husb_3',\n",
    "                        'C(occupation_husb)[T.4.0]':'occ_husb_4',\n",
    "                        'C(occupation_husb)[T.5.0]':'occ_husb_5',\n",
    "                        'C(occupation_husb)[T.6.0]':'occ_husb_6'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>occ_2</th>\n",
       "      <th>occ_3</th>\n",
       "      <th>occ_4</th>\n",
       "      <th>occ_5</th>\n",
       "      <th>occ_6</th>\n",
       "      <th>occ_husb_2</th>\n",
       "      <th>occ_husb_3</th>\n",
       "      <th>occ_husb_4</th>\n",
       "      <th>occ_husb_5</th>\n",
       "      <th>occ_husb_6</th>\n",
       "      <th>rate_marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>yrs_married</th>\n",
       "      <th>children</th>\n",
       "      <th>religious</th>\n",
       "      <th>educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept  occ_2  occ_3  occ_4  occ_5  occ_6  occ_husb_2  occ_husb_3  \\\n",
       "0        1.0    1.0    0.0    0.0    0.0    0.0         0.0         0.0   \n",
       "1        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3        1.0    0.0    0.0    0.0    1.0    0.0         0.0         0.0   \n",
       "4        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "\n",
       "   occ_husb_4  occ_husb_5  occ_husb_6  rate_marriage   age  yrs_married  \\\n",
       "0         0.0         1.0         0.0            3.0  32.0          9.0   \n",
       "1         1.0         0.0         0.0            3.0  27.0         13.0   \n",
       "2         0.0         1.0         0.0            4.0  22.0          2.5   \n",
       "3         0.0         1.0         0.0            4.0  37.0         16.5   \n",
       "4         1.0         0.0         0.0            5.0  27.0          9.0   \n",
       "\n",
       "   children  religious  educ  \n",
       "0       3.0        3.0  17.0  \n",
       "1       3.0        1.0  14.0  \n",
       "2       0.0        1.0  16.0  \n",
       "3       4.0        3.0  16.0  \n",
       "4       1.0        1.0  14.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to flatten `y` into a 1-D array, so that scikit-learn will properly understand it as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# flatten y into a 1-D array\n",
    "y = np.ravel(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258875274897895"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73% accuracy seems good, but what's the null error rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3224945020420987"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage had affairs?\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 32% of the women had affairs, which means that you could obtain 68% accuracy by always predicting \"no\". So we're doing better than the null error rate, but not by much.\n",
    "\n",
    "Let's examine the coefficients to see what we learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Intercept', 'occ_2', 'occ_3', 'occ_4', 'occ_5', 'occ_6', 'occ_husb_2',\n",
       "        'occ_husb_3', 'occ_husb_4', 'occ_husb_5', 'occ_husb_6', 'rate_marriage',\n",
       "        'age', 'yrs_married', 'children', 'religious', 'educ'],\n",
       "       dtype='object'), array([[ 1.48983606],\n",
       "        [ 0.1880664 ],\n",
       "        [ 0.49894812],\n",
       "        [ 0.25066814],\n",
       "        [ 0.839008  ],\n",
       "        [ 0.83390825],\n",
       "        [ 0.19063622],\n",
       "        [ 0.2978329 ],\n",
       "        [ 0.16140914],\n",
       "        [ 0.18777109],\n",
       "        [ 0.19401647],\n",
       "        [-0.70311973],\n",
       "        [-0.05841813],\n",
       "        [ 0.10567669],\n",
       "        [ 0.01691983],\n",
       "        [-0.37113512],\n",
       "        [ 0.00401584]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the coefficients\n",
    "X.columns, np.transpose(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increases in marriage rating and religiousness correspond to a decrease in the likelihood of having an affair. For both, wife's occupation and the husband's occupation, the lowest likelihood of having an affair corresponds to the baseline occupation (student), since all of the dummy coefficients are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Using a Validation Set\n",
    "\n",
    "So far, we have trained and tested on the same set. Let's instead split the data into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Intercept  occ_2  occ_3  occ_4  occ_5  occ_6  occ_husb_2  occ_husb_3  \\\n",
       "2411        1.0    1.0    0.0    0.0    0.0    0.0         0.0         0.0   \n",
       "4083        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3196        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3035        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "1772        1.0    0.0    0.0    0.0    1.0    0.0         0.0         0.0   \n",
       "5880        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "1237        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "1850        1.0    0.0    0.0    0.0    1.0    0.0         0.0         0.0   \n",
       "507         1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "4809        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "88          1.0    1.0    0.0    0.0    0.0    0.0         1.0         0.0   \n",
       "2809        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "4609        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "470         1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "5679        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "2355        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3359        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "5482        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "5373        1.0    1.0    0.0    0.0    0.0    0.0         1.0         0.0   \n",
       "2270        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "9           1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "439         1.0    1.0    0.0    0.0    0.0    0.0         1.0         0.0   \n",
       "5264        1.0    0.0    1.0    0.0    0.0    0.0         1.0         0.0   \n",
       "4629        1.0    1.0    0.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3037        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3890        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "4958        1.0    0.0    0.0    0.0    0.0    1.0         0.0         0.0   \n",
       "1082        1.0    1.0    0.0    0.0    0.0    0.0         1.0         0.0   \n",
       "34          1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "5269        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "...         ...    ...    ...    ...    ...    ...         ...         ...   \n",
       "2008        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "99          1.0    0.0    0.0    0.0    1.0    0.0         1.0         0.0   \n",
       "2496        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "1871        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "2046        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "4851        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "5072        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2163        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "6036        1.0    1.0    0.0    0.0    0.0    0.0         1.0         0.0   \n",
       "6216        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2893        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "537         1.0    0.0    0.0    0.0    1.0    0.0         1.0         0.0   \n",
       "1701        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2897        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "2222        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2135        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "2599        1.0    1.0    0.0    0.0    0.0    0.0         0.0         1.0   \n",
       "705         1.0    0.0    0.0    0.0    1.0    0.0         0.0         0.0   \n",
       "3468        1.0    0.0    0.0    1.0    0.0    0.0         1.0         0.0   \n",
       "5924        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "5874        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "4373        1.0    0.0    0.0    0.0    1.0    0.0         0.0         0.0   \n",
       "1033        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "5827        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "4859        1.0    0.0    0.0    1.0    0.0    0.0         0.0         0.0   \n",
       "4931        1.0    1.0    0.0    0.0    0.0    0.0         0.0         0.0   \n",
       "3264        1.0    1.0    0.0    0.0    0.0    0.0         1.0         0.0   \n",
       "1653        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2607        1.0    0.0    1.0    0.0    0.0    0.0         0.0         0.0   \n",
       "2732        1.0    0.0    1.0    0.0    0.0    0.0         1.0         0.0   \n",
       "\n",
       "      occ_husb_4  occ_husb_5  occ_husb_6  rate_marriage   age  yrs_married  \\\n",
       "2411         0.0         0.0         1.0            5.0  27.0          6.0   \n",
       "4083         0.0         1.0         0.0            4.0  42.0         23.0   \n",
       "3196         0.0         1.0         0.0            5.0  37.0         23.0   \n",
       "3035         1.0         0.0         0.0            5.0  22.0          2.5   \n",
       "1772         1.0         0.0         0.0            3.0  22.0          2.5   \n",
       "5880         1.0         0.0         0.0            2.0  27.0          2.5   \n",
       "1237         0.0         1.0         0.0            3.0  32.0         16.5   \n",
       "1850         1.0         0.0         0.0            4.0  32.0         13.0   \n",
       "507          1.0         0.0         0.0            5.0  27.0          9.0   \n",
       "4809         0.0         1.0         0.0            5.0  32.0         16.5   \n",
       "88           0.0         0.0         0.0            3.0  42.0         23.0   \n",
       "2809         0.0         1.0         0.0            5.0  27.0          6.0   \n",
       "4609         0.0         1.0         0.0            4.0  32.0         13.0   \n",
       "470          1.0         0.0         0.0            2.0  42.0         23.0   \n",
       "5679         0.0         0.0         1.0            5.0  32.0          2.5   \n",
       "2355         1.0         0.0         0.0            5.0  32.0          2.5   \n",
       "3359         0.0         1.0         0.0            5.0  32.0         16.5   \n",
       "5482         0.0         1.0         0.0            4.0  27.0          6.0   \n",
       "5373         0.0         0.0         0.0            4.0  27.0          6.0   \n",
       "2270         1.0         0.0         0.0            5.0  32.0         16.5   \n",
       "9            0.0         1.0         0.0            3.0  27.0          6.0   \n",
       "439          0.0         0.0         0.0            3.0  32.0         16.5   \n",
       "5264         0.0         0.0         0.0            5.0  32.0         16.5   \n",
       "4629         1.0         0.0         0.0            5.0  27.0         13.0   \n",
       "3037         1.0         0.0         0.0            4.0  22.0          2.5   \n",
       "3890         0.0         0.0         0.0            5.0  17.5          0.5   \n",
       "4958         1.0         0.0         0.0            4.0  37.0         13.0   \n",
       "1082         0.0         0.0         0.0            4.0  32.0         13.0   \n",
       "34           0.0         0.0         1.0            5.0  42.0         16.5   \n",
       "5269         0.0         1.0         0.0            5.0  27.0          6.0   \n",
       "...          ...         ...         ...            ...   ...          ...   \n",
       "2008         0.0         1.0         0.0            4.0  27.0          9.0   \n",
       "99           0.0         0.0         0.0            4.0  27.0          2.5   \n",
       "2496         0.0         0.0         1.0            5.0  27.0          6.0   \n",
       "1871         1.0         0.0         0.0            3.0  22.0          2.5   \n",
       "2046         1.0         0.0         0.0            3.0  27.0          6.0   \n",
       "4851         0.0         0.0         0.0            5.0  17.5          2.5   \n",
       "5072         0.0         0.0         1.0            5.0  22.0          2.5   \n",
       "2163         1.0         0.0         0.0            3.0  22.0          2.5   \n",
       "6036         0.0         0.0         0.0            5.0  22.0          2.5   \n",
       "6216         0.0         1.0         0.0            4.0  27.0          6.0   \n",
       "2893         1.0         0.0         0.0            5.0  22.0          0.5   \n",
       "537          0.0         0.0         0.0            2.0  37.0         23.0   \n",
       "1701         1.0         0.0         0.0            4.0  27.0          6.0   \n",
       "2897         0.0         0.0         1.0            5.0  27.0          6.0   \n",
       "2222         0.0         1.0         0.0            5.0  42.0         23.0   \n",
       "2135         1.0         0.0         0.0            5.0  32.0          9.0   \n",
       "2599         0.0         0.0         0.0            5.0  32.0         16.5   \n",
       "705          0.0         1.0         0.0            4.0  27.0          2.5   \n",
       "3468         0.0         0.0         0.0            5.0  32.0         13.0   \n",
       "5924         1.0         0.0         0.0            4.0  22.0          2.5   \n",
       "5874         1.0         0.0         0.0            5.0  22.0          2.5   \n",
       "4373         1.0         0.0         0.0            5.0  27.0          2.5   \n",
       "1033         0.0         0.0         1.0            5.0  27.0          6.0   \n",
       "5827         1.0         0.0         0.0            3.0  27.0          9.0   \n",
       "4859         0.0         0.0         0.0            4.0  27.0          2.5   \n",
       "4931         1.0         0.0         0.0            3.0  27.0          2.5   \n",
       "3264         0.0         0.0         0.0            4.0  17.5          0.5   \n",
       "1653         1.0         0.0         0.0            3.0  22.0          2.5   \n",
       "2607         0.0         1.0         0.0            5.0  22.0          0.5   \n",
       "2732         0.0         0.0         0.0            4.0  22.0          2.5   \n",
       "\n",
       "      children  religious  educ  \n",
       "2411       1.0        2.0  16.0  \n",
       "4083       4.0        3.0  14.0  \n",
       "3196       4.0        2.0  14.0  \n",
       "3035       0.0        2.0  12.0  \n",
       "1772       0.0        3.0  14.0  \n",
       "5880       1.0        2.0  14.0  \n",
       "1237       4.0        2.0  12.0  \n",
       "1850       2.0        2.0  14.0  \n",
       "507        1.0        3.0  14.0  \n",
       "4809       3.0        1.0  14.0  \n",
       "88         2.0        2.0  12.0  \n",
       "2809       2.0        2.0  17.0  \n",
       "4609       2.0        4.0  14.0  \n",
       "470        5.5        3.0  17.0  \n",
       "5679       1.0        1.0  14.0  \n",
       "2355       0.0        3.0  14.0  \n",
       "3359       2.0        1.0  14.0  \n",
       "5482       1.0        1.0  14.0  \n",
       "5373       0.0        2.0  14.0  \n",
       "2270       2.0        3.0  12.0  \n",
       "9          0.0        1.0  16.0  \n",
       "439        3.0        2.0  12.0  \n",
       "5264       2.0        3.0  12.0  \n",
       "4629       2.0        2.0  14.0  \n",
       "3037       0.0        2.0  12.0  \n",
       "3890       0.0        3.0  12.0  \n",
       "4958       1.0        4.0  20.0  \n",
       "1082       1.0        3.0  20.0  \n",
       "34         4.0        3.0  16.0  \n",
       "5269       0.0        2.0  16.0  \n",
       "...        ...        ...   ...  \n",
       "2008       2.0        2.0  16.0  \n",
       "99         1.0        2.0  12.0  \n",
       "2496       1.0        3.0  20.0  \n",
       "1871       0.0        3.0  16.0  \n",
       "2046       0.0        1.0  12.0  \n",
       "4851       0.0        4.0  12.0  \n",
       "5072       2.0        1.0  16.0  \n",
       "2163       0.0        2.0  14.0  \n",
       "6036       0.0        3.0  14.0  \n",
       "6216       0.0        2.0  14.0  \n",
       "2893       0.0        2.0  16.0  \n",
       "537        4.0        2.0  12.0  \n",
       "1701       0.0        3.0  14.0  \n",
       "2897       0.0        2.0  20.0  \n",
       "2222       3.0        3.0  14.0  \n",
       "2135       2.0        3.0  16.0  \n",
       "2599       2.0        1.0  12.0  \n",
       "705        0.0        2.0  14.0  \n",
       "3468       2.0        2.0  16.0  \n",
       "5924       0.0        3.0  14.0  \n",
       "5874       0.0        3.0  12.0  \n",
       "4373       0.0        1.0  20.0  \n",
       "1033       1.0        2.0  14.0  \n",
       "5827       2.0        2.0  14.0  \n",
       "4859       0.0        2.0  16.0  \n",
       "4931       1.0        2.0  14.0  \n",
       "3264       0.0        2.0  12.0  \n",
       "1653       0.0        2.0  14.0  \n",
       "2607       0.0        2.0  14.0  \n",
       "2732       0.0        1.0  12.0  \n",
       "\n",
       "[4456 rows x 17 columns]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model2.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35142488, 0.64857512],\n",
       "       [0.90952576, 0.09047424],\n",
       "       [0.72576603, 0.27423397],\n",
       "       ...,\n",
       "       [0.55736751, 0.44263249],\n",
       "       [0.81213933, 0.18786067],\n",
       "       [0.74729595, 0.25270405]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate class probabilities\n",
    "probs = model2.predict_proba(X_test)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifier is predicting a 1 (having an affair) any time the probability in the second column is greater than 0.5.\n",
    "\n",
    "Now let's generate some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7298429319371728\n",
      "0.7459619860896347\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print(metrics.accuracy_score(y_test, predicted))\n",
    "print(metrics.roc_auc_score(y_test, probs[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is 73%, which is the same as we experienced when training and predicting on the same data.\n",
    "\n",
    "We can also see the confusion matrix and a classification report with other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1169  134]\n",
      " [ 382  225]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.90      0.82      1303\n",
      "        1.0       0.63      0.37      0.47       607\n",
      "\n",
      "avg / total       0.71      0.73      0.71      1910\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1910"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "1169+134+382+225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Using Cross-Validation\n",
    "\n",
    "Now let's try 10-fold cross-validation, to see if the accuracy holds up more rigorously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.72100313, 0.70219436, 0.73824451, 0.70597484, 0.70597484,\n",
       "        0.72955975, 0.7327044 , 0.70440252, 0.75157233, 0.75      ]),\n",
       " 0.7241630685514876)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "scores, scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. It's still performing at 73% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Probability of an Affair\n",
    "\n",
    "Just for fun, let's predict the probability of an affair for a random woman not present in the dataset. She's a 25-year-old teacher who graduated from college, has been married for 3 years. She has 1 child, rates herself as strongly religious, rates her marriage as fair, and her husband is a farmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7747227, 0.2252773]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(np.array([[1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 25, 3, 1, 4,\n",
    "                              16]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted probability of an affair is 23%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
