{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality-reduction technique that is often used to transform a high-dimensional dataset into a smaller-dimensional subspace prior to running a machine learning algorithm on the data.\n",
    "\n",
    "## When should you use PCA?\n",
    "\n",
    "It is often helpful to use a dimensionality-reduction technique such as PCA prior to performing machine learning because:\n",
    "\n",
    "   * Reducing the dimensionality of the dataset reduces the size of the space on which k-nearest-neighbors (kNN) must calculate distance, which improve the performance of kNN.\n",
    "   * Reducing the dimensionality of the dataset reduces the number of degrees of freedom of the hypothesis, which reduces the risk of overfitting.\n",
    "   * Most algorithms will run significantly faster if they have fewer dimensions they need to look at.\n",
    "   * Reducing the dimensionality via PCA can simplify the dataset, facilitating description, visualization, and insight.\n",
    "\n",
    "\n",
    "## What does PCA do?\n",
    "\n",
    "Principal Component Analysis does just what it advertises; it finds the principal components of the dataset. PCA transforms the data into a new, lower-dimensional subspace—into a new coordinate system—. In the new coordinate system, the first axis corresponds to the first principal component, which is the component that explains the greatest amount of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      "[[ 3  7]\n",
      " [-4 -6]\n",
      " [ 7  8]\n",
      " [ 1 -1]\n",
      " [-4 -1]\n",
      " [-3 -7]]\n",
      "---\n",
      "Dimensions:\n",
      "(6, 2)\n",
      "---\n",
      "Mean across Rows:\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create (6x2) array\n",
    "A = np.array([\n",
    "        [ 3,  7],\n",
    "        [-4, -6],\n",
    "        [ 7,  8],\n",
    "        [ 1, -1],\n",
    "        [-4, -1],\n",
    "        [-3, -7]\n",
    "    ])\n",
    "\n",
    "m,n = A.shape # m-observations, n-features\n",
    "\n",
    "print(\"Array:\")\n",
    "print(A) # our array\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Dimensions:\")\n",
    "print(A.shape) # shape\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Mean across Rows:\")\n",
    "print(np.mean(A,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a0  a1\n",
      "0   3   7\n",
      "1  -4  -6\n",
      "2   7   8\n",
      "3   1  -1\n",
      "4  -4  -1\n",
      "5  -3  -7\n"
     ]
    }
   ],
   "source": [
    "# Note: you can convert this easily into a DataFrame ...\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(A, columns = ['a0', 'a1'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  7],\n",
       "       [-4, -6],\n",
       "       [ 7,  8],\n",
       "       [ 1, -1],\n",
       "       [-4, -1],\n",
       "       [-3, -7]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... and can go from df back to np.array\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# makes charts pretty\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Sara\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Dataset $A$')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots\n",
    "plt.scatter(A[:,0],A[:,1])\n",
    "\n",
    "# annotations\n",
    "for i in range(m):\n",
    "    plt.annotate('('+str(A[i,0])+','+str(A[i,1])+')',(A[i,0]+0.2,A[i,1]+0.2))\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Dataset $A$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample covariance between $a_0$ and $a_1$:\n",
    "\n",
    "$$\n",
    "cov_{a_0,a_1} =\\frac{\\sum_{i=0}^{m-1}(a_0^i - \\bar{a_0})(a_1^i - \\bar{a_1})}{m-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of prod equals 6\n",
      "---\n",
      "Covariance:\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate covariance between a0 and a1\n",
    "a0 = A[:,0]\n",
    "a1 = A[:,1]\n",
    "prod = a0*a1 # element-wise product, ignore means as zero already\n",
    "print(\"Length of prod equals \" + str(len(prod)))\n",
    "print(\"---\")\n",
    "print(\"Covariance:\")\n",
    "print(np.sum(prod)/(m-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20., 25.],\n",
       "       [25., 40.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get more stuff using NumPy's covariance method\n",
    "np.cov(a0,a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebra way:\n",
    "$$\n",
    "\\Sigma = \\frac{A^TA}{(m-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, -4,  7,  1, -4, -3],\n",
       "       [ 7, -6,  8, -1, -1, -7]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aside: What does A.T do?\n",
    "A.T # or np.transpose(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100, 125],\n",
       "       [125, 200]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication, note @ operator\n",
    "A.T @ A # or np.dot(A.T,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20., 25.],\n",
       "       [25., 40.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to divide by (m-1) to yield true Sample Covariance Matrix\n",
    "# Let's call this Sigma\n",
    "Sigma = (A.T @ A)/(m-1) # or np.cov(A.T)\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Eigen-decomposition of $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Wikipedia article on PCA](https://en.m.wikipedia.org/wiki/Principal_component_analysis), *\"PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix.\"* I choose the first approach.\n",
    "\n",
    "*$\\Sigma$ is a real, symmetric matrix; thus, it has 1) real eigenvalues and 2) orthogonal eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[ 3.07417596 56.92582404]\n",
      "---\n",
      "Eigenvectors:\n",
      "[[-0.82806723 -0.56062881]\n",
      " [ 0.56062881 -0.82806723]]\n"
     ]
    }
   ],
   "source": [
    "l, X = np.linalg.eig(Sigma)\n",
    "print(\"Eigenvalues:\")\n",
    "print(l)\n",
    "print(\"---\")\n",
    "print(\"Eigenvectors:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from your Linear Algebra class that the following should hold:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Sigma x_0 &=& \\lambda_0 x_0 \\nonumber \\\\\n",
    "\\Sigma x_1 &=& \\lambda_1 x_1 \\nonumber \\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma times eigenvector:\n",
      "[-2.54562438  1.72347161]\n",
      "Eigenvalue times eigenvector:\n",
      "[-2.54562438  1.72347161]\n"
     ]
    }
   ],
   "source": [
    "# let's check the first Eigenvalue, Eigenvector combination\n",
    "print(\"Sigma times eigenvector:\")\n",
    "print(Sigma @ X[:,0]) # 2x2 times 2x1\n",
    "print(\"Eigenvalue times eigenvector:\")\n",
    "print(l[0] * X[:,0]) # scalar times 2x1, ANNOYING - MUST USE * vs. @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma times eigenvector:\n",
      "[-31.91425695 -47.13840945]\n",
      "Eigenvalue times eigenvector:\n",
      "[-31.91425695 -47.13840945]\n"
     ]
    }
   ],
   "source": [
    "# ... and the second\n",
    "print(\"Sigma times eigenvector:\")\n",
    "print(Sigma @ X[:,1]) # 2x2 times 2x1\n",
    "print(\"Eigenvalue times eigenvector:\")\n",
    "print(l[1] * X[:,1]) # scalar times 2x1, ANNOYING - MUST USE * vs. @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first principal component is evector with largest evalue:\n",
      "[-0.56062881 -0.82806723]\n",
      "---\n",
      "Second principal component:\n",
      "[-0.82806723  0.56062881]\n"
     ]
    }
   ],
   "source": [
    "print(\"The first principal component is evector with largest evalue:\")\n",
    "print(X[:,1])\n",
    "print(\"---\")\n",
    "print(\"Second principal component:\")\n",
    "print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Orthogonal? A: Yes\n",
    "X[:,1].T @ X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Length 1? A: Yes\n",
    "print(np.sqrt(X[:,1].T @ X[:,1]))\n",
    "print(np.sqrt(X[:,0].T @ X[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Sara\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Eigenvectors of $\\\\Sigma$')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots\n",
    "plt.scatter(A[:,0],A[:,1])\n",
    "scale = 3 # increase this scaling factor to highlight these vectors\n",
    "plt.plot([0,X[0,1]*scale],[0,X[1,1]*scale],'r') # First principal component\n",
    "plt.plot([0,X[0,0]*scale],[0,X[1,0]*scale],'g') # Second principal component\n",
    "\n",
    "# annotations\n",
    "for i in range(m):\n",
    "    plt.annotate('('+str(A[i,0])+','+str(A[i,1])+')',(A[i,0]+0.2,A[i,1]+0.2))\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Eigenvectors of $\\Sigma$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dimensionality Reduction: 2D to 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I discovered why Prof. Ng recommended **Octave/Matlab** versus **Python**. Linear algebra expressions are not clean in **Python**. Had to convert `np.array` to `matrix` object. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to matrix\n",
    "Amat = np.asmatrix(A)\n",
    "Xmat = np.asmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose eigenvector with highest eigenvalue as first principal component\n",
    "pc1 = Xmat[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed version of A:\n",
      "[[ -7.47835704]\n",
      " [  7.21091862]\n",
      " [-10.54893951]\n",
      " [  0.26743842]\n",
      " [  3.07058247]\n",
      " [  7.47835704]]\n"
     ]
    }
   ],
   "source": [
    "Acomp = Amat @ pc1 # 6x2 @ 2x1 yields 6x1\n",
    "print(\"Compressed version of A:\")\n",
    "print(Acomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction from 1D compression of A:\n",
      "[[ 4.1925824   6.1925824 ]\n",
      " [-4.04264872 -5.97112541]\n",
      " [ 5.9140394   8.73523112]\n",
      " [-0.14993368 -0.22145699]\n",
      " [-1.72145699 -2.54264872]\n",
      " [-4.1925824  -6.1925824 ]]\n"
     ]
    }
   ],
   "source": [
    "Arec = Acomp @ pc1.T # 6x1 @ 1x2, this breaks with np.array\n",
    "print(\"Reconstruction from 1D compression of A:\")\n",
    "print(Arec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-2dd2f6267e0f>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-2dd2f6267e0f>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    Principal Component Analysis is a very powerful unsupervised method for *dimensionality reduction* in data.  It's easiest to visualize by looking at a two-dimensional dataset:\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "        \n",
    "# import print_function, division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting style defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "# plots\n",
    "plt.scatter([Amat[:,0]], [Amat[:,1]]\n",
    "            \n",
    "# Dimensionality Reduction: Principal Component Analysis in-depth\n",
    "\n",
    "# Here we'll explore **Principal Component Analysis**, which is an extremely useful linear dimensionality reduction technique.\n",
    "# We'll start with our standard set of initial imports:\n",
    "\n",
    "\n",
    "\n",
    "## Introducing Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis is a very powerful unsupervised method for *dimensionality reduction* in data.  It's easiest to visualize by looking at a two-dimensional dataset:\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
    "plt.plot(X[:, 0], X[:, 1], 'o')\n",
    "plt.axis('equal');\n",
    "\n",
    "We can see that there is a definite trend in the data. What PCA seeks to do is to find the **Principal Axes** in the data, and explain how important those axes are in describing the data distribution:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.components_)\n",
    "\n",
    "To see what these numbers mean, let's view them as vectors plotted on top of the data:\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
    "plt.axis('equal');\n",
    "\n",
    "Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more \"important\" than the other direction.\n",
    "The explained variance quantifies this measure of \"importance\" in direction.\n",
    "\n",
    "Another way to think of it is that the second principal component could be **completely ignored** without much loss of information! Let's see what our data look like if we only keep 95% of the variance:\n",
    "\n",
    "clf = PCA(0.95) # keep 95% of variance\n",
    "X_trans = clf.fit_transform(X)\n",
    "print(X.shape)\n",
    "print(X_trans.shape)\n",
    "\n",
    "By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let's see what the data look like after this compression:\n",
    "\n",
    "X_new = clf.inverse_transform(X_trans)\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\n",
    "plt.axis('equal');\n",
    "\n",
    "The light points are the original data, while the dark points are the projected version.  We see that after truncating 5% of the variance of this dataset and then reprojecting it, the \"most important\" features of the data are maintained, and we've compressed the data by 50%!\n",
    "\n",
    "This is the sense in which \"dimensionality reduction\" works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data.\n",
    "\n",
    "### Application of PCA to Digits\n",
    "\n",
    "The dimensionality reduction might seem a bit abstract in two dimensions, but the projection and dimensionality reduction can be extremely useful when visualizing high-dimensional data.  Let's take a quick look at the application of PCA to the digits data we looked at before:\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "Xproj = pca.fit_transform(X)\n",
    "print(X.shape)\n",
    "print(Xproj.shape)\n",
    "\n",
    "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar();\n",
    "\n",
    "This gives us an idea of the relationship between the digits. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits, **without reference** to the labels.\n",
    "\n",
    "### What do the Components Mean?\n",
    "\n",
    "PCA is a very useful dimensionality reduction algorithm, because it has a very intuitive interpretation via *eigenvectors*.\n",
    "The input data is represented as a vector: in the case of the digits, our data is\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, x_3 \\cdots]\n",
    "$$\n",
    "\n",
    "but what this really means is\n",
    "\n",
    "$$\n",
    "image(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots\n",
    "$$\n",
    "\n",
    "If we reduce the dimensionality in the pixel space to (say) 6, we recover only a partial image:\n",
    "\n",
    "from fig_code.figures import plot_image_components\n",
    "\n",
    "sns.set_style('white')\n",
    "plot_image_components(digits.data[0])\n",
    "\n",
    "But the pixel-wise representation is not the only choice. We can also use other *basis functions*, and write something like\n",
    "\n",
    "$$\n",
    "image(x) = {\\rm mean} + x_1 \\cdot{\\rm (basis~1)} + x_2 \\cdot{\\rm (basis~2)} + x_3 \\cdot{\\rm (basis~3)} \\cdots\n",
    "$$\n",
    "\n",
    "What PCA does is to choose optimal **basis functions** so that only a few are needed to get a reasonable approximation.\n",
    "The low-dimensional representation of our data is the coefficients of this series, and the approximate reconstruction is the result of the sum:\n",
    "\n",
    "from fig_code.figures import plot_pca_interactive\n",
    "plot_pca_interactive(digits.data)\n",
    "\n",
    "Here we see that with only six PCA components, we recover a reasonable approximation of the input!\n",
    "\n",
    "Thus we see that PCA can be viewed from two angles. It can be viewed as **dimensionality reduction**, or it can be viewed as a form of **lossy data compression** where the loss favors noise. In this way, PCA can be used as a **filtering** process as well.\n",
    "\n",
    "### Choosing the Number of Components\n",
    "\n",
    "But how much information have we thrown away?  We can figure this out by looking at the **explained variance** as a function of the components:\n",
    "\n",
    "sns.set()\n",
    "pca = PCA().fit(X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.\n",
    "\n",
    "### PCA as data compression\n",
    "\n",
    "As we mentioned, PCA can be used for is a sort of data compression. Using a small ``n_components`` allows you to represent a high dimensional point as a sum of just a few principal vectors.\n",
    "\n",
    "Here's what a single digit looks like as you change the number of components:\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pca = PCA(i + 1).fit(X)\n",
    "    im = pca.inverse_transform(pca.transform(X[20:21]))\n",
    "\n",
    "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "Let's take another look at this by using IPython's ``interact`` functionality to view the reconstruction of several images at once:\n",
    "\n",
    "from IPython.html.widgets import interact\n",
    "\n",
    "def plot_digits(n_components):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 1, 1, frameon=False, xticks=[], yticks=[])\n",
    "    nside = 10\n",
    "    \n",
    "    pca = PCA(n_components).fit(X)\n",
    "    Xproj = pca.inverse_transform(pca.transform(X[:nside ** 2]))\n",
    "    Xproj = np.reshape(Xproj, (nside, nside, 8, 8))\n",
    "    total_var = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    im = np.vstack([np.hstack([Xproj[i, j] for j in range(nside)])\n",
    "                    for i in range(nside)])\n",
    "    plt.imshow(im)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"n = {0}, variance = {1:.2f}\".format(n_components, total_var),\n",
    "                 size=18)\n",
    "    plt.clim(0, 16)\n",
    "    \n",
    "interact(plot_digits, n_components=[1, 64], nside=[1, 8]);) # A in blue\n",
    "plt.plot(Arec[:,0],Arec[:,1],'r', marker='o') # Arec in RED\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Reconstructing the 1D compression of $A$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.matrix_rank(Amat)) # originally a Rank 2 matrix\n",
    "print(np.linalg.matrix_rank(Arec)) # reconstructed matrix is Rank 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tacking on the Rank-1 matrix related to the 2nd eigenvector you get back to the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Rank 1 matrix for the other vector to recover A completely\n",
    "Amat @ Xmat[:,1] @ Xmat[:,1].T + Amat @ Xmat[:,0] @ Xmat[:,0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why does this work? Well, recall \n",
    "# X @ X.T is identity matrix as X is orthonormal\n",
    "A @ Xmat @ Xmat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "plt.scatter(A[:,0], A[:,1]) # A in blue\n",
    "plt.plot(Arec[:,0],Arec[:,1],'r', marker='o') # Arec in RED\n",
    "\n",
    "# across observations\n",
    "for i in range(m):\n",
    "    e = np.vstack((A[i],Arec[i]))\n",
    "    plt.plot(e[:,0],e[:,1],'b') # BLUE\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Back to $A$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wicked animated GIF which illustrates PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "\n",
    "Magically, eigen-decomposition (or PCA) finds the line where\n",
    "1. the spread of values along the black line is **maximal**\n",
    "2. the projection error (sum of red lines) is **minimal**\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/Q7HIP.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Variance Retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average squared projection error using PC1\n",
    "unexp_err = np.mean(np.sum(np.square(Amat - Arec),axis=1))\n",
    "total_err = np.mean(np.sum(np.square(Amat),axis=1))\n",
    "ret_err = 1 - (unexp_err / total_err) # percent of variance retained\n",
    "print(ret_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using eigenvalues\n",
    "l[1]/np.sum(l) # recall, use the 2nd eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Summary of Eigen-decomposition Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalize columns of $A$ so that each feature has zero mean\n",
    "1. Compute sample covariance matrix $\\Sigma = {A^TA}/{(m-1)}$\n",
    "1. Perform eigen-decomposition of $\\Sigma$ using `np.linalg.eig(Sigma)`\n",
    "1. Compress by ordering $k$ evectors according to largest evalues and compute $AX_k$\n",
    "1. Reconstruct from compressed version by computing $A X_k X_k^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try going from 4D to 2D using the classical iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() # Bunch object\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = iris.data # np.array\n",
    "\n",
    "print(\"Dimensions:\")\n",
    "print(A0.shape)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"First 5 samples:\")\n",
    "print(A0[:5,:])\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Feature names:\")\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen-decomposition: 5-step process\n",
    "\n",
    "# 1. Normalize columns of $A$ so that each feature has zero mean\n",
    "mu = np.mean(A0,axis=0)\n",
    "A = A0 - mu\n",
    "print(\"Does A have zero mean across rows?\")\n",
    "print(np.mean(A,axis=0))\n",
    "\n",
    "# 2. Compute sample covariance matrix $\\Sigma = {A^TA}/{(m-1)}$\n",
    "m,n = A.shape\n",
    "Sigma = (A.T @ A)/(m-1)\n",
    "print(\"---\")\n",
    "print(\"Sigma:\")\n",
    "print(Sigma)\n",
    "\n",
    "# 3. Perform eigen-decomposition of $\\Sigma$ using `np.linalg.eig(Sigma)`\n",
    "l,X = np.linalg.eig(Sigma)\n",
    "print(\"---\")\n",
    "print(\"Evalues:\")\n",
    "print(l)\n",
    "print(\"---\")\n",
    "print(\"Evectors:\")\n",
    "print(X)\n",
    "\n",
    "# 4. Compress by ordering $k$ evectors according to largest evalues and compute $AX_k$\n",
    "print(\"---\")\n",
    "print(\"Compressed - 4D to 2D:\")\n",
    "Acomp = A @ X[:,:2] # first 2 evectors\n",
    "print(Acomp[:5,:]) # first 5 observations\n",
    "\n",
    "# 5. Reconstruct from compressed version by computing $A X_k X_k^T$\n",
    "print(\"---\")\n",
    "print(\"Reconstructed version - 2D to 4D:\")\n",
    "Arec = A @ X[:,:2] @ X[:,:2].T # first 2 evectors\n",
    "print(Arec[:5,:]+mu) # first 5 obs, adding mu to compare to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using sklearn.decomposition.PCA\n",
    "\n",
    "pca = PCA(n_components=2) # two components\n",
    "pca.fit(A0) # run PCA, putting in raw version for fun\n",
    "\n",
    "print(\"Principal components:\")\n",
    "print(pca.components_)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Compressed - 4D to 2D:\")\n",
    "print(pca.transform(A0)[:5,:]) # first 5 obs\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Reconstructed - 2D to 4D:\")\n",
    "print(pca.inverse_transform(pca.transform(A0))[:5,:]) # first 5 obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting style defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
    "plt.plot(X[:, 0], X[:, 1], 'o')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a definite trend in the data. What PCA seeks to do is to find the **Principal Axes** in the data, and explain how important those axes are in describing the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what these numbers mean, let's view them as vectors plotted on top of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more \"important\" than the other direction.\n",
    "The explained variance quantifies this measure of \"importance\" in direction.\n",
    "\n",
    "Another way to think of it is that the second principal component could be **completely ignored** without much loss of information! Let's see what our data look like if we only keep 95% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PCA(0.95) # keep 95% of variance\n",
    "X_trans = clf.fit_transform(X)\n",
    "print(X.shape)\n",
    "print(X_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let's see what the data look like after this compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = clf.inverse_transform(X_trans)\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light points are the original data, while the dark points are the projected version.  We see that after truncating 5% of the variance of this dataset and then reprojecting it, the \"most important\" features of the data are maintained, and we've compressed the data by 50%!\n",
    "\n",
    "This is the sense in which \"dimensionality reduction\" works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
